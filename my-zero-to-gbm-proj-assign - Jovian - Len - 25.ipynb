{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# my-zero-to-gbm-proj-assign"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this competition I am taking a running competiton in Kaggle which is supposed by end by Sept 28 2021. I am trying to participate in that competition.  Below is my own program written based on the foundations of the stock market volataluty computations help provided by Optiver company to all participants as the domain is not known to all. The base given as a \"book example\". After that everything else is my self work in determing the Imputing strategy, Feature engineering for Order data and trade data "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optiver Realized Volatility Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This dataset contains stock market data relevant to the practical execution of trades in the financial markets. In particular, it includes order book snapshots and executed trades. With one second resolution, it provides a uniquely fine grained look at the micro-structure of modern financial markets.\r\n",
    "\r\n",
    "This is a code competition where only the first few rows of the test set are available for download. The rows that are visible are intended to illustrate the hidden test set format and folder structure. The remainder will only be available to your notebook when it is submitted. The hidden test set contains data that can be used to construct features to predict roughly 150,000 target values. Loading the entire dataset will take slightly more than 3 GB of memory, by our estimation.\r\n",
    "\r\n",
    "This is also a forecasting competition, where the final private leaderboard will be determined using data gathered after the training period closes, which means that the public and private leaderboards will have zero overlap. During the active training stage of the competition a large fraction of the test data will be filler, intended only to ensure the hidden dataset has approximately the same size as the actual test data. The filler data will be removed entirely during the forecasting phase of the competition and replaced with real"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install jovian --upgrade --quiet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import jovian"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Description\r\n",
    "\r\n",
    "book_[train/test].parquet A parquet file partitioned by stock_id. Provides order book data on the most competitive buy and sell orders entered into the market. The top two levels of the book are shared. The first level of the book will be more competitive in price terms, it will then receive execution priority over the second level.\r\n",
    "\r\n",
    "stock_id - ID code for the stock. Not all stock IDs exist in every time bucket. Parquet coerces this column to the categorical data type when loaded; you may wish to convert it to int8.\r\n",
    "time_id - ID code for the time bucket. Time IDs are not necessarily sequential but are consistent across all stocks.\r\n",
    "seconds_in_bucket - Number of seconds from the start of the bucket, always starting from 0.\r\n",
    "bid_price[1/2] - Normalized prices of the most/second most competitive buy level.\r\n",
    "ask_price[1/2] - Normalized prices of the most/second most competitive sell level.\r\n",
    "bid_size[1/2] - The number of shares on the most/second most competitive buy level.\r\n",
    "ask_size[1/2] - The number of shares on the most/second most competitive sell level.\r\n",
    "trade_[train/test].parquet A parquet file partitioned by stock_id. Contains data on trades that actually executed. Usually, in the market, there are more passive buy/sell intention updates (book updates) than actual trades, therefore one may expect this file to be more sparse than the order book.\r\n",
    "\r\n",
    "stock_id - Same as above.\r\n",
    "time_id - Same as above.\r\n",
    "seconds_in_bucket - Same as above. Note that since trade and book data are taken from the same time window and trade data is more sparse in general, this field is not necessarily starting from 0.\r\n",
    "price - The average price of executed transactions happening in one second. Prices have been normalized and the average has been weighted by the number of shares traded in each transaction.\r\n",
    "size - The sum number of shares traded.\r\n",
    "order_count - The number of unique trade orders taking place.\r\n",
    "train.csv The ground truth values for the training set.\r\n",
    "\r\n",
    "stock_id - Same as above, but since this is a csv the column will load as an integer instead of categorical.\r\n",
    "time_id - Same as above.\r\n",
    "target - The realized volatility computed over the 10 minute window following the feature data under the same stock/time_id. There is no overlap between feature and target data. You can find more info in our tutorial notebook.\r\n",
    "test.csv Provides the mapping between the other data files and the submission file. As with other test files, most of the data is only available to your notebook upon submission with just the first few rows available for download.\r\n",
    "\r\n",
    "stock_id - Same as above.\r\n",
    "time_id - Same as above.\r\n",
    "row_id - Unique identifier for the submission row. There is one row for each existing time ID/stock ID pair. Each time window is not necessarily containing every individual stock.\r\n",
    "sample_submission.csv - A sample submission file in the correct format.\r\n",
    "\r\n",
    "row_id - Same as in test.csv.\r\n",
    "target - Same definition as in train.csv. The benchmark is using the median target value from train.csv."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Execute this to save new versions of the notebook\r\n",
    "#jovian.commit(project=\"my-zero-to-gbm-proj-assign\")\r\n",
    "jovian.commit(filename=\"my-zero-to-gbm-proj-assign.ipynb\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import plotly.express as px\r\n",
    "#train = pd.read_csv('../../../data/optiver-realized-volatility-prediction/train.csv')\r\n",
    "train = pd.read_csv('c:\\\\optiver-realized-volatility-prediction\\\\train.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "train.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 428932 entries, 0 to 428931\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   stock_id  428932 non-null  int64  \n",
      " 1   time_id   428932 non-null  int64  \n",
      " 2   target    428932 non-null  float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 9.8 MB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Points to be addressed before ML modeling\r\n",
    "\r\n",
    "1. stock_id - ID code for the stock. <font color='red'> Not all stock IDs exist in every time bucket </font>. Parquet coerces this column to the categorical data type when loaded;  <font color='red'> you may wish to convert it to int8 </font>\r\n",
    "2. We have missing “seconds_in_bucket” field?.A: That means there is no related market activities during the last second. For book data you can also assume the top-2 level book shape stays the same as the last available book update within the gap seconds, or, in another word, <font color='red'> you can forward fill the missing data point for all field in book data.</font>\r\n",
    "3. I'm trying to make trade data fixed sized. Since missing seconds_in_bucket implies no trade happening within that one-second window, is it technically correct to resample trade data to 600 seconds and fill it with zeros?. Hi, it is correct to assume 0 for order count and size. Some assumptions are required for the price, though. A price of 0 might cause issues.\r\n",
    "4. the trade data at T seconds contains a 1-second aggregation of executed orders between [T, T+1 second]\r\n",
    "5. One time_id represents a unique 20-minutes trading window which is consistent across all stocks As an example, let’s say time_id = 1 is representing a window between 1900-01-01 12:00:00 and 1900-01-01 12:20:00, then the book data of all stocks for that time_id are is taken from the same window. The data in the first 10 minutes window is shared with all of you, while the order book data of the second 10-minutes is used to build the target for you to predict. The dataset is rolling in such a way that feature and target data will always have zero overlap. Note that time_id is randomly shuffled, so it will not contain any information other than serving as a bridge between different dataset.\r\n",
    "\r\n",
    "\r\n",
    "6. We can demonstrate the data structure in below way:\r\n",
    "<img src = https://www.optiver.com/wp-content/uploads/2021/05/DataBucketing.png>\r\n",
    "\r\n",
    "\r\n",
    "7. In our competition, we shared the last snapshot of order book for each second. Imagine you have a time_id starting from 1900-01-01 12:00:00, the book update data on seconds_in_bucket = 1 represents the last snapshot of order book update between 12:00:00 and 12:00:01. Similarly to order book data in terms of granularity, but the trade data represents the aggregation of all individual orders happened within one second.\r\n",
    "8. So per stock, under the same time_id, the trade data on seconds_in_bucket = 1 represents the aggregation of all individual executed orders between 12:00:00 and 12:00:01. The size is the sum of the size in each individual order, while the price is aggregated as a volume weighted average price of all trades. A straightforward WAP formula can be found on Investopedia.\r\n",
    "9. Q: Why we have missing “seconds_in_bucket” field?\r\n",
    "A: That means there is no related market activities during the last second.\r\n",
    "\r\n",
    "For book data you can also assume the top-2 level book shape stays the same as the last available book update within the gap seconds, or, in another word, you can forward fill the missing data point for all field in book data. For trade data, it implies no trade happening within that one-second window. One thing to note that trade data tends to be more sparse than book data in many cases.\r\n",
    "\r\n",
    "10. \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Taking the first row of data, it implies that the realized vol of the target bucket for time_id 5, stock_id 0 is 0.004136. How does the book and trade data in feature bucket look like for us to build signals?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "book_example = pd.read_parquet('c:\\\\optiver-realized-volatility-prediction\\\\book_train.parquet/stock_id=0')\r\n",
    "trade_example =  pd.read_parquet('c:\\\\optiver-realized-volatility-prediction\\\\trade_train.parquet/stock_id=0')\r\n",
    "stock_id = '0'\r\n",
    "book_example = book_example[book_example['time_id']==5]\r\n",
    "book_example.loc[:,'stock_id'] = stock_id\r\n",
    "trade_example = trade_example[trade_example['time_id']==5]\r\n",
    "trade_example.loc[:,'stock_id'] = stock_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Book data snapshot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "book_example.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 302 entries, 0 to 301\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   time_id            302 non-null    int16  \n",
      " 1   seconds_in_bucket  302 non-null    int16  \n",
      " 2   bid_price1         302 non-null    float32\n",
      " 3   ask_price1         302 non-null    float32\n",
      " 4   bid_price2         302 non-null    float32\n",
      " 5   ask_price2         302 non-null    float32\n",
      " 6   bid_size1          302 non-null    int32  \n",
      " 7   ask_size1          302 non-null    int32  \n",
      " 8   bid_size2          302 non-null    int32  \n",
      " 9   ask_size2          302 non-null    int32  \n",
      " 10  stock_id           302 non-null    object \n",
      "dtypes: float32(4), int16(2), int32(4), object(1)\n",
      "memory usage: 15.3+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trade Book Snapshot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "trade_example.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 40 entries, 0 to 39\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   time_id            40 non-null     int16  \n",
      " 1   seconds_in_bucket  40 non-null     int16  \n",
      " 2   price              40 non-null     float32\n",
      " 3   size               40 non-null     int32  \n",
      " 4   order_count        40 non-null     int16  \n",
      " 5   stock_id           40 non-null     object \n",
      "dtypes: float32(1), int16(3), int32(1), object(1)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Realized volatility calculation\r\n",
    "\r\n",
    "our target is to predict short-term realized volatility. Although the order book and trade data for the target cannot be shared, we can still present the realized volatility calculation using the feature data we provided.\r\n",
    "\r\n",
    "As realized volatility is a statistical measure of price changes on a given stock, to calculate the price change we first need to have a stock valuation at the fixed interval (1 second). We will use weighted averaged price, or WAP, of the order book data we provided."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "book_example['bid_size'] = (book_example['bid_size1'] + book_example['bid_size2'])\r\n",
    "book_example['bid_price'] = (book_example['bid_price1'] * book_example['bid_size1'] + book_example['bid_price2'] * book_example['bid_size2'])/book_example['bid_size']\r\n",
    "\r\n",
    "book_example['ask_size'] = (book_example['ask_size1'] + book_example['ask_size2'])\r\n",
    "book_example['ask_price'] = (book_example['ask_price1'] * book_example['ask_size1'] + book_example['ask_price2'] * book_example['ask_size2'])/(book_example['ask_size1'] + book_example['ask_size2'])\r\n",
    "\r\n",
    "book_example['wap'] = (book_example['bid_price'] * book_example['ask_size'] + book_example['ask_price'] * book_example['bid_size']) / (book_example['bid_size'] +  book_example['ask_size'])\r\n",
    "\r\n",
    "book_example['wap1'] = (book_example['bid_price1'] * book_example['ask_size1'] + book_example['ask_price1'] * book_example['bid_size1']) / (book_example['bid_size1'] +  book_example['ask_size1'])\r\n",
    "\r\n",
    "book_example['wap2'] = (book_example['bid_price2'] * book_example['ask_size2'] + book_example['ask_price2'] * book_example['bid_size2']) / (book_example['bid_size2'] +  book_example['ask_size2'])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The WAP of the stock is plotted below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "fig = px.line(book_example, x=\"seconds_in_bucket\", y=\"wap\", title='WAP of stock_id_0, time_id_5')\r\n",
    "fig.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "seconds_in_bucket=%{x}<br>wap=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          1,
          5,
          6,
          7,
          11,
          12,
          14,
          15,
          16,
          17,
          18,
          19,
          21,
          24,
          25,
          44,
          46,
          47,
          48,
          50,
          51,
          55,
          57,
          59,
          60,
          63,
          68,
          69,
          78,
          79,
          81,
          83,
          85,
          90,
          92,
          93,
          94,
          97,
          98,
          101,
          103,
          105,
          106,
          110,
          111,
          113,
          114,
          115,
          118,
          119,
          122,
          123,
          124,
          126,
          127,
          128,
          129,
          132,
          133,
          134,
          138,
          140,
          141,
          143,
          144,
          145,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          158,
          159,
          160,
          161,
          164,
          165,
          166,
          167,
          168,
          169,
          171,
          173,
          174,
          175,
          177,
          178,
          181,
          182,
          183,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          203,
          204,
          205,
          206,
          207,
          209,
          211,
          212,
          213,
          215,
          216,
          218,
          219,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          230,
          231,
          232,
          233,
          234,
          235,
          237,
          239,
          248,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          263,
          264,
          265,
          269,
          278,
          282,
          283,
          287,
          288,
          290,
          293,
          295,
          296,
          298,
          301,
          303,
          304,
          305,
          306,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          316,
          322,
          323,
          329,
          331,
          336,
          338,
          340,
          345,
          348,
          350,
          353,
          356,
          363,
          365,
          366,
          367,
          374,
          378,
          380,
          382,
          385,
          386,
          388,
          389,
          391,
          394,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          406,
          410,
          411,
          416,
          425,
          426,
          427,
          428,
          430,
          431,
          433,
          434,
          435,
          437,
          438,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          464,
          465,
          466,
          467,
          470,
          471,
          472,
          473,
          474,
          475,
          477,
          478,
          479,
          488,
          489,
          495,
          496,
          498,
          503,
          511,
          513,
          514,
          515,
          516,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          528,
          531,
          532,
          534,
          535,
          538,
          539,
          540,
          541,
          542,
          545,
          546,
          548,
          549,
          550,
          551,
          552,
          555,
          558,
          561,
          562,
          563,
          571,
          572,
          573,
          574,
          575,
          579,
          581,
          584,
          585,
          586,
          587,
          588,
          593
         ],
         "xaxis": "x",
         "y": [
          1.0014153049230474,
          1.0014240509126244,
          1.001424681849596,
          1.001421940664183,
          1.001421940664183,
          1.001424681849596,
          1.001421940664183,
          1.001421940664183,
          1.001421940664183,
          1.001421940664183,
          1.001424681849596,
          1.001421940664183,
          1.001421940664183,
          1.001456901286742,
          1.0022547684273755,
          1.0021900320694048,
          1.0021949093209948,
          1.0036719808998917,
          1.0035453765131508,
          1.002943939641532,
          1.0025254854676318,
          1.0025119290037643,
          1.002696280554202,
          1.003021316121993,
          1.0031591807636777,
          1.0032391784446166,
          1.0028941207524489,
          1.0032716624817186,
          1.003249723875096,
          1.0038702978267813,
          1.004104370973389,
          1.004109053152231,
          1.004104370973389,
          1.0041072125098978,
          1.0041940164197272,
          1.004192775395139,
          1.0041544236302684,
          1.004129878706812,
          1.0041624463266063,
          1.0041612313982733,
          1.004128925791115,
          1.0038910299746533,
          1.0039773563991727,
          1.0039142464196937,
          1.0039185521948768,
          1.0039207317466539,
          1.0039284566247682,
          1.004026431960969,
          1.0040914533151457,
          1.003909617246145,
          1.0039020840534914,
          1.0043248506867017,
          1.0042948024867695,
          1.0042018521409015,
          1.0048736549850203,
          1.0045556010980379,
          1.0049375111117538,
          1.0049164838174258,
          1.0049215160685478,
          1.0047186351329942,
          1.0048485292890459,
          1.0048472715490426,
          1.0045862741561482,
          1.0044961542205106,
          1.0045727206174968,
          1.0043924467443033,
          1.0043911133698316,
          1.0040098129719397,
          1.0037038588338474,
          1.0036263692147203,
          1.003730479543736,
          1.0040184984265805,
          1.0038469259073821,
          1.0038372950862824,
          1.0038358314308597,
          1.0038469259073821,
          1.0037689754729393,
          1.0038469259073821,
          1.003797998883898,
          1.0038277857774858,
          1.0038291106787511,
          1.0038277857774858,
          1.0038291106787511,
          1.0038327222550634,
          1.0036579724502426,
          1.003657098334076,
          1.0037224508787315,
          1.0036579724502426,
          1.003657098334076,
          1.003766109615437,
          1.003918220719078,
          1.0038163103893223,
          1.0038174279179917,
          1.003918220719078,
          1.0038622715242473,
          1.0038630560332655,
          1.004404357126618,
          1.0043061525839494,
          1.0041239034355378,
          1.004022849977766,
          1.0039056947125227,
          1.003911050706128,
          1.0039109661688213,
          1.0039120562451642,
          1.0039109661688213,
          1.0038610998671003,
          1.0040335779142853,
          1.0038651960730047,
          1.0038610148312046,
          1.0039650573323804,
          1.0039664282392888,
          1.0041576217187098,
          1.0040905171517442,
          1.0040883608528857,
          1.003952029685883,
          1.0036620271481302,
          1.0036265948360057,
          1.0042106913589928,
          1.0039848249823575,
          1.0039545257451863,
          1.0039835104864592,
          1.0039492230641758,
          1.0040973533902848,
          1.004201663408533,
          1.0042036484515071,
          1.004110218195026,
          1.004070480939687,
          1.004318656072892,
          1.0042690391429976,
          1.0043556364432291,
          1.00437158498928,
          1.0042221364538348,
          1.004001127052082,
          1.004163443278285,
          1.0041647083472724,
          1.00424738149042,
          1.004311493575914,
          1.0041775659688612,
          1.0041522539458514,
          1.0041833854116677,
          1.0041821544867702,
          1.0041243005736766,
          1.0039163572975138,
          1.0039144876844994,
          1.0036720585526506,
          1.0044525134295983,
          1.0042832116089244,
          1.0042319870231176,
          1.0042701563224417,
          1.003848520799603,
          1.0041977630937649,
          1.0037994543161481,
          1.0039934784758324,
          1.0040163704711866,
          1.0038016476851543,
          1.0035629155669343,
          1.0036299582556651,
          1.0035285318104423,
          1.0035285607732773,
          1.0035366105364383,
          1.0035529188513812,
          1.0035530322937165,
          1.0035483350189789,
          1.003834198969897,
          1.0039112445621308,
          1.003670362808467,
          1.003574485368486,
          1.0037407681787303,
          1.0036372568564205,
          1.0043141255794328,
          1.0040687864850215,
          1.0040668527829375,
          1.0044553174439825,
          1.004186774564598,
          1.004088416365063,
          1.00394019542001,
          1.0042628337617827,
          1.0037980161461175,
          1.0036839876911723,
          1.0040599301803943,
          1.004051355824518,
          1.004051355824518,
          1.0036787412967554,
          1.003656318653656,
          1.00368525329047,
          1.003656318653656,
          1.00368525329047,
          1.003656318653656,
          1.00368525329047,
          1.003656318653656,
          1.0039567586293066,
          1.0039608233853392,
          1.0039903142445146,
          1.0039699422347057,
          1.0045365036067784,
          1.0043270914378113,
          1.0043374949479607,
          1.0042640536758565,
          1.0042627729368552,
          1.0044091159325017,
          1.0044094315460697,
          1.0044394063993074,
          1.0043020204370003,
          1.0042888374642944,
          1.0042838578702604,
          1.0042747788297057,
          1.0042326134868471,
          1.0044614540734227,
          1.004386485185714,
          1.0043852329309304,
          1.0043851181008476,
          1.0043851181008476,
          1.0043650319795585,
          1.0038512545175984,
          1.0041553758527253,
          1.0041912705922733,
          1.0041923465308602,
          1.0041490770734807,
          1.0040380580294255,
          1.0039298240113976,
          1.0039570989841196,
          1.0041044578600908,
          1.0039989166414207,
          1.0040455931569896,
          1.0040690584959415,
          1.003959292637175,
          1.0040104221823625,
          1.0040624357166916,
          1.0040101125802048,
          1.0040878808487776,
          1.004016639929173,
          1.0040178112692715,
          1.0037207775437869,
          1.0037097380586857,
          1.0039889843459797,
          1.00399238514409,
          1.003863650462239,
          1.0038626593678837,
          1.0034147795674684,
          1.0036310235053365,
          1.003448580776145,
          1.0034248432114556,
          1.003503262539683,
          1.0039406279582248,
          1.0038532254742643,
          1.003926597510674,
          1.0039406279582248,
          1.0040088119669857,
          1.0040060356259346,
          1.0040172449107438,
          1.0040106534683526,
          1.0040117803471758,
          1.004020162815774,
          1.00392328750095,
          1.0039241907204883,
          1.0038653405371816,
          1.0040177999627897,
          1.0040301455983915,
          1.0040224883790223,
          1.0035122305378374,
          1.0035120471508585,
          1.003587719391404,
          1.0035436307915677,
          1.0035403395471303,
          1.003540107462659,
          1.0034486584364737,
          1.0036020083948556,
          1.0036214455940942,
          1.0036307684577432,
          1.0035317720930077,
          1.0038330325003568,
          1.0038301538159493,
          1.003512415522132,
          1.0035080766176077,
          1.0037139660120011,
          1.0034562191449328,
          1.0033271033794453,
          1.0031182382488708,
          1.0031184019045618,
          1.0030226207536619,
          1.0027250732060267,
          1.00279976594041,
          1.0020280055382154,
          1.0020281098985884,
          1.0020281098985884,
          1.002383034267005,
          1.0025581261690926,
          1.0025519213234608,
          1.002479840330599,
          1.0026325868060848,
          1.003359691401852,
          1.0036870971596055,
          1.0032509488541537,
          1.0034315039340713,
          1.0034352219435199,
          1.0036979037472324,
          1.0034813904067128,
          1.0037448844868384,
          1.0037387605586083,
          1.0037448844868384,
          1.0037387605586083,
          1.0037448844868384
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "WAP of stock_id_0, time_id_5"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "seconds_in_bucket"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "wap"
         }
        }
       }
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Log returns\n",
    "\n",
    "How can we compare the price of a stock between yesterday and today?\n",
    "\n",
    "The easiest method would be to just take the difference. This is definitely the most intuitive way, however price differences are not always comparable across stocks. For example, let's assume that we have invested \\$ 1000 in both stock A and stock B and that stock A moves from  \\$ 100 to  \\$ 102 and stock B moves from  \\$ 10 to  \\$ 11. We had a total of 10 shares of A ( \\$1000 / \\$100=10 ) which led to a profit of  10⋅(\\$102−\\$100)=\\$20  and a total of 100 shares of B that yielded \\$100. So the price increase was larger for stock A, although the move was proportionally much larger for stock B.\n",
    "\n",
    "We can solve the above problem by dividing the move by the starting price of the stock, effectively computing the percentage change in price, also known as the stock return. In our example, the return for stock A was  \\$102−\\$100/\\$100=2% , while for stock B it was  \\$11−\\$10/\\$10=10% . The stock return coincides with the percentage change in our invested capital.\n",
    "\n",
    "Returns are widely used in finance, however log returns are preferred whenever some mathematical modelling is required. Calling  St  the price of the stock  S  at time  t , we can define the log return between  t1  and  t2  as:\n",
    "\n",
    " - r(t1,t2)=log(St2/St1)\n",
    " \n",
    "Usually, we look at log returns over fixed time intervals, so with 10-minute log return we mean  rt=rt−10min,t .\n",
    "\n",
    "Log returns present several advantages, for example:\n",
    "\n",
    "- they are additive across time  r(t1,t2) + r(t2,t3) = (rt1,t3) \n",
    "- regular returns cannot go below -100%, while log returns are not bounded\n",
    "\n",
    "Next we will compute the log return\n",
    "\n",
    "To compute the Log Return, we will simply take the logarithm of the ratio between two consecutive WAP. The first row will have an empty return as the previous book update is unknown, therefore the empty return data point will be dropped."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def log_return(list_stock_prices):\r\n",
    "    return np.log(list_stock_prices).diff()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "book_example.loc[:,'log_return'] = log_return(book_example['wap'])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "book_example['log_return'] = book_example['log_return'].fillna(book_example['log_return'].mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "book_example.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>bid_size</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>ask_size</th>\n",
       "      <th>ask_price</th>\n",
       "      <th>wap</th>\n",
       "      <th>wap1</th>\n",
       "      <th>wap2</th>\n",
       "      <th>log_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.001422</td>\n",
       "      <td>1.002301</td>\n",
       "      <td>1.00137</td>\n",
       "      <td>1.002353</td>\n",
       "      <td>3</td>\n",
       "      <td>226</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.001401</td>\n",
       "      <td>326</td>\n",
       "      <td>1.002317</td>\n",
       "      <td>1.001415</td>\n",
       "      <td>1.001434</td>\n",
       "      <td>1.001390</td>\n",
       "      <td>7.719553e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.001422</td>\n",
       "      <td>1.002301</td>\n",
       "      <td>1.00137</td>\n",
       "      <td>1.002353</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.001401</td>\n",
       "      <td>200</td>\n",
       "      <td>1.002327</td>\n",
       "      <td>1.001424</td>\n",
       "      <td>1.001448</td>\n",
       "      <td>1.001390</td>\n",
       "      <td>8.733591e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.001422</td>\n",
       "      <td>1.002301</td>\n",
       "      <td>1.00137</td>\n",
       "      <td>1.002405</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.001401</td>\n",
       "      <td>200</td>\n",
       "      <td>1.002353</td>\n",
       "      <td>1.001425</td>\n",
       "      <td>1.001448</td>\n",
       "      <td>1.001391</td>\n",
       "      <td>6.300396e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1.001422</td>\n",
       "      <td>1.002301</td>\n",
       "      <td>1.00137</td>\n",
       "      <td>1.002405</td>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.001401</td>\n",
       "      <td>226</td>\n",
       "      <td>1.002347</td>\n",
       "      <td>1.001422</td>\n",
       "      <td>1.001443</td>\n",
       "      <td>1.001391</td>\n",
       "      <td>-2.737289e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1.001422</td>\n",
       "      <td>1.002301</td>\n",
       "      <td>1.00137</td>\n",
       "      <td>1.002405</td>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.001401</td>\n",
       "      <td>226</td>\n",
       "      <td>1.002347</td>\n",
       "      <td>1.001422</td>\n",
       "      <td>1.001443</td>\n",
       "      <td>1.001391</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_id  seconds_in_bucket  bid_price1  ask_price1  bid_price2  ask_price2  \\\n",
       "0        5                  0    1.001422    1.002301     1.00137    1.002353   \n",
       "1        5                  1    1.001422    1.002301     1.00137    1.002353   \n",
       "2        5                  5    1.001422    1.002301     1.00137    1.002405   \n",
       "3        5                  6    1.001422    1.002301     1.00137    1.002405   \n",
       "4        5                  7    1.001422    1.002301     1.00137    1.002405   \n",
       "\n",
       "   bid_size1  ask_size1  bid_size2  ask_size2 stock_id  bid_size  bid_price  \\\n",
       "0          3        226          2        100        0         5   1.001401   \n",
       "1          3        100          2        100        0         5   1.001401   \n",
       "2          3        100          2        100        0         5   1.001401   \n",
       "3          3        126          2        100        0         5   1.001401   \n",
       "4          3        126          2        100        0         5   1.001401   \n",
       "\n",
       "   ask_size  ask_price       wap      wap1      wap2    log_return  \n",
       "0       326   1.002317  1.001415  1.001434  1.001390  7.719553e-06  \n",
       "1       200   1.002327  1.001424  1.001448  1.001390  8.733591e-06  \n",
       "2       200   1.002353  1.001425  1.001448  1.001391  6.300396e-07  \n",
       "3       226   1.002347  1.001422  1.001443  1.001391 -2.737289e-06  \n",
       "4       226   1.002347  1.001422  1.001443  1.001391  0.000000e+00  "
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#~ means NOT \r\n",
    "# also the null condition is applied because when we .diff() method the first row will have empty value as we cannot have diff of return with respect to the previous time id as the first row is the very first time id\r\n",
    "book_example = book_example[~book_example['log_return'].isnull()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "book_example['log_return'].isnull().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's plot the tick-to-tick return of this instrument over this time bucket"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "fig = px.line(book_example, x=\"seconds_in_bucket\", y=\"log_return\", title='Log return of stock_id_0, time_id_5')\r\n",
    "fig.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "seconds_in_bucket=%{x}<br>log_return=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          1,
          5,
          6,
          7,
          11,
          12,
          14,
          15,
          16,
          17,
          18,
          19,
          21,
          24,
          25,
          44,
          46,
          47,
          48,
          50,
          51,
          55,
          57,
          59,
          60,
          63,
          68,
          69,
          78,
          79,
          81,
          83,
          85,
          90,
          92,
          93,
          94,
          97,
          98,
          101,
          103,
          105,
          106,
          110,
          111,
          113,
          114,
          115,
          118,
          119,
          122,
          123,
          124,
          126,
          127,
          128,
          129,
          132,
          133,
          134,
          138,
          140,
          141,
          143,
          144,
          145,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          158,
          159,
          160,
          161,
          164,
          165,
          166,
          167,
          168,
          169,
          171,
          173,
          174,
          175,
          177,
          178,
          181,
          182,
          183,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          203,
          204,
          205,
          206,
          207,
          209,
          211,
          212,
          213,
          215,
          216,
          218,
          219,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          230,
          231,
          232,
          233,
          234,
          235,
          237,
          239,
          248,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          263,
          264,
          265,
          269,
          278,
          282,
          283,
          287,
          288,
          290,
          293,
          295,
          296,
          298,
          301,
          303,
          304,
          305,
          306,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          316,
          322,
          323,
          329,
          331,
          336,
          338,
          340,
          345,
          348,
          350,
          353,
          356,
          363,
          365,
          366,
          367,
          374,
          378,
          380,
          382,
          385,
          386,
          388,
          389,
          391,
          394,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          406,
          410,
          411,
          416,
          425,
          426,
          427,
          428,
          430,
          431,
          433,
          434,
          435,
          437,
          438,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          464,
          465,
          466,
          467,
          470,
          471,
          472,
          473,
          474,
          475,
          477,
          478,
          479,
          488,
          489,
          495,
          496,
          498,
          503,
          511,
          513,
          514,
          515,
          516,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          528,
          531,
          532,
          534,
          535,
          538,
          539,
          540,
          541,
          542,
          545,
          546,
          548,
          549,
          550,
          551,
          552,
          555,
          558,
          561,
          562,
          563,
          571,
          572,
          573,
          574,
          575,
          579,
          581,
          584,
          585,
          586,
          587,
          588,
          593
         ],
         "xaxis": "x",
         "y": [
          0.00000771955329291023,
          0.00000873359069121591,
          6.300395643831815e-7,
          -0.0000027372893980680615,
          0,
          0.0000027372893980680615,
          -0.0000027372893980680615,
          0,
          0,
          0,
          0.0000027372893980680615,
          -0.0000027372893980680615,
          0,
          0.00003491037184098311,
          0.0007963892159363938,
          -0.00006459280692302106,
          0.000004866581752062393,
          0.0014727516098684626,
          -0.0001261491551388461,
          -0.0005994917439021315,
          -0.000417312949012262,
          -0.00001352240488863973,
          0.0001838727271096189,
          0.0003241090083419794,
          0.00013743991840792398,
          0.00007974257017721257,
          -0.00034400275964577114,
          0.0003763813906925177,
          -0.00002186730405440932,
          0.0006183725584422055,
          0.0002331435264647688,
          0.000004663029124030681,
          -0.000004663029124030681,
          0.000002829917457035022,
          0.00008644510956130023,
          -0.0000012358422125293561,
          -0.00003819236523826264,
          -0.000024443674064514136,
          0.00003243314670001857,
          -0.0000012098929535351785,
          -0.000032172250653538306,
          -0.0002369456706758843,
          0.00008598813065485884,
          -0.00006286193877317464,
          0.000004288977833257802,
          0.0000021710420694447466,
          0.00000769467961943271,
          0.0000975871886720412,
          0.00006475850314974305,
          -0.00018111152644644402,
          -0.00000750388360505486,
          0.00042103472684516504,
          -0.000029919253141838,
          -0.00009255713293838605,
          0.000668768164038369,
          -0.00031656142028955395,
          0.00038010582469141796,
          -0.000020924200844216548,
          0.000005007618646203291,
          -0.00020190772574043506,
          0.00012927575503051195,
          -0.0000012516720219379537,
          -0.0002597721083336833,
          -0.00008971253187538709,
          0.0000762207786723177,
          -0.00017946938713142395,
          -0.000001327544189626273,
          -0.0003797054636571883,
          -0.0003047786591854713,
          -0.0000772066479972737,
          0.00010372877067029319,
          0.00028690726577915073,
          -0.0001709004174649015,
          -0.000009593960045204807,
          -0.000001458061477546766,
          0.000011052021522751573,
          -0.00007765472910269957,
          0.00007765472910269957,
          -0.00004874071394750986,
          0.000029673750773119914,
          0.0000013198482945006758,
          -0.0000013198482945006758,
          0.0000013198482945006758,
          0.0000035977934660052883,
          -0.00017409774871991355,
          -8.709307066766953e-7,
          0.00006511229498593583,
          -0.00006424136427925914,
          -8.709307066766953e-7,
          0.00010860817094926261,
          0.0001515289048518811,
          -0.0001015177338081312,
          0.000001113279427474717,
          0.00010040445438065648,
          -0.000055732382161577734,
          7.814903835940204e-7,
          0.0005390727379369804,
          -0.00009777869162530973,
          -0.0001814841879761598,
          -0.0001006434989727998,
          -0.00011669266387591111,
          0.000005335141882373013,
          -8.420796863452262e-8,
          0.0000010858291102353902,
          -0.0000010858291102353902,
          -0.00004967326976482861,
          0.00017179989520149132,
          -0.00016771945265037485,
          -0.0000041651513811242005,
          0.00010363696608123394,
          0.0000013654917195024888,
          0.00019041998924588296,
          -0.00006682895972037403,
          -0.0000021475167199481657,
          -0.00013578528385628074,
          -0.00028890267906094653,
          -0.00003530365463159631,
          0.0005818166078296978,
          -0.00022494460897825707,
          -0.000030179434615452104,
          0.00002887015510627533,
          -0.00003415196307507591,
          0.0001475367435344035,
          0.000103878971676373,
          0.0000019767354353184344,
          -0.00009304348100172295,
          -0.000039575378213157,
          0.00024713849511991213,
          -0.00004940479325123842,
          0.00008622546694534317,
          0.000015879255163253267,
          -0.00014880912364257943,
          -0.00022010441434599636,
          0.0001616562994434011,
          0.0000012598229889079077,
          0.00008232687266979902,
          0.00006383889096974339,
          -0.00013336155018114568,
          -0.000025207037967728968,
          0.00003100225401447398,
          -0.0000012257976679174928,
          -0.00005761462645680147,
          -0.00020711062407128677,
          -0.0000018623212399374728,
          -0.00024151300822528856,
          0.0007772973122604572,
          -0.00016856554992719045,
          -0.00005100741667561984,
          0.00003800772576235782,
          -0.0004199308873904361,
          0.00034784287641618367,
          -0.0003967224450187115,
          0.00019327108599633946,
          0.00002280068035244876,
          -0.00021388670179187759,
          -0.00023785626558912474,
          0.00006680243800194887,
          -0.00010106470994805184,
          2.8860997652955384e-8,
          0.000008021426783788848,
          0.000016250710000224617,
          1.1304070444148953e-7,
          -0.000004680655211667002,
          0.00028481263337722237,
          0.00007674836719952964,
          -0.00023997206782055795,
          -0.00009553138486425227,
          0.00016567682662773807,
          -0.00010313087132626493,
          0.0006741883838136515,
          -0.0002443150598004573,
          -0.000001925868000348524,
          0.0003868164082499251,
          -0.00026738748735065897,
          -0.00009795291009713405,
          -0.0001476283200869226,
          0.00032132044405899524,
          -0.0004629517317548621,
          -0.00011360346428999826,
          0.0003744924741340668,
          -0.000008539721813238636,
          0,
          -0.0003711799036882936,
          -0.000022340707884472427,
          0.000028828812415878334,
          -0.000028828812415878334,
          0.000028828812415878334,
          -0.000028828812415878334,
          0.000028828812415878334,
          -0.000028828812415878334,
          0.0002993006782959981,
          0.000004048727964798236,
          0.000029374080499084375,
          -0.000020291248040224183,
          0.0005641618808570328,
          -0.00020848819227169395,
          0.000010358633512000913,
          -0.00007312677040125296,
          -0.0000012753018623678017,
          0.0001457112002377168,
          3.142280504565137e-7,
          0.000029842816118138757,
          -0.0001367881009330836,
          -0.000013126588378362114,
          -0.000004958340861230953,
          -0.000009040354001984066,
          -0.00004198674401310106,
          0.00022785011541708294,
          -0.00007463868845253643,
          -0.0000012467865535461392,
          -1.1432873122350917e-7,
          0,
          -0.000019998625801207967,
          -0.0005116754371768434,
          0.0003029086984600083,
          0.000035745561768806174,
          0.0000010714472849894865,
          -0.00004308974248868041,
          -0.00011056643300586151,
          -0.00010780453123847723,
          0.000027167837402826585,
          0.00014676728981095812,
          -0.00010511532434778498,
          0.0000464895228661855,
          0.00002337051715017857,
          -0.00010932700109929361,
          0.000050926609919646655,
          0.00005180442944207296,
          -0.000052112794971537375,
          0.00007745465496825522,
          -0.00007095339799039098,
          0.0000011666533886135408,
          -0.00029588884659265775,
          -0.000010998622383669252,
          0.0002781754911516639,
          0.000003387280541450842,
          -0.00012823098842200094,
          -9.87280338360201e-7,
          -0.00044625600742621493,
          0.00021548480694740533,
          -0.00018179919650893918,
          -0.000023656264921153962,
          0.00007814861732462035,
          0.00043574361159724913,
          -0.00008706320508586069,
          0.00007308773163508531,
          0.000013975473450775384,
          0.0000679140693797542,
          -0.0000027652594843993045,
          0.000011164496864164535,
          -0.000006565090451760708,
          0.0000011223767266404366,
          0.000008348939494658321,
          -0.00009649207489776258,
          8.996893918191903e-7,
          -0.00005862186489910418,
          0.00015186085567030433,
          0.000012296156204101918,
          -0.000007626512611981669,
          -0.000508342742334477,
          -1.8274515258681623e-7,
          0.00007540456318394029,
          -0.00004393195277365477,
          -0.000003279628043543082,
          -2.3126573879235848e-7,
          -0.00009113058109991735,
          0.00015281124808115857,
          0.00001936725001948445,
          0.000009289180087960168,
          -0.00009864309722385205,
          0.0003001551177160759,
          -0.000002867696560993928,
          -0.0003165760556512725,
          -0.0000043237271780101685,
          0.0002051485991513267,
          -0.0002568261230573763,
          -0.00012867932897031326,
          -0.00020819418998917812,
          1.6314694659992635e-7,
          -0.00009548795412775467,
          -0.00029669489409260693,
          0.00007448697015355202,
          -0.000769901984894304,
          1.0414915251749646e-7,
          0,
          0.00035414328358655215,
          0.00017466039002816828,
          -0.000006189032506739682,
          -0.00007190010080779154,
          0.00015235701870282975,
          0.0007249326286137476,
          0.0003262562313180589,
          -0.00043464053430472636,
          0.0001799538139719394,
          0.000003705287850559475,
          0.00026174826358059617,
          -0.00021573891479453053,
          0.0002625454687744501,
          -0.000006101099000438599,
          0.000006101099000438599,
          -0.000006101099000438599,
          0.000006101099000438599
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Log return of stock_id_0, time_id_5"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "seconds_in_bucket"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "log_return"
         }
        }
       }
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Realized volatility\r\n",
    "\r\n",
    "When we trade options, a valuable input to our models is the standard deviation of the stock log returns. The standard deviation will be different for log returns computed over longer or shorter intervals, for this reason it is usually normalized to a 1-year period and the annualized standard deviation is called volatility.\r\n",
    "\r\n",
    "In this competition, you will be given 10 minutes of book data and we ask you to predict what the volatility will be in the following 10 minutes. Volatility will be measured as follows:\r\n",
    "\r\n",
    "We will compute the log returns over all consecutive book updates and we define the realized volatility,  σ , as the squared root of the sum of squared log returns.\r\n",
    "\r\n",
    "\\sigma = \\sqrt{\\sum_{t}r_{t-1, t}^2}\r\n",
    " \r\n",
    "Where we use WAP as price of the stock to compute log returns.\r\n",
    "\r\n",
    "We want to keep definitions as simple and clear as possible, so that Kagglers without financial knowledge will not be penalized. So we are not annualizing the volatility and we are assuming that log returns have 0 mean.\r\n",
    "\r\n",
    "\r\n",
    "The realized vol of stock 0 in this feature bucket, will be:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def realized_volatility(series_log_return):\r\n",
    "    return np.sqrt(np.sum(series_log_return**2))\r\n",
    "\r\n",
    "realized_vol = realized_volatility(book_example['log_return'])\r\n",
    "print(f'Realized volatility for stock_id 0 on time_id 5 is {realized_vol}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Realized volatility for stock_id 0 on time_id 5 is 0.0037818447045652633\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive prediction: using past realized volatility as target\n",
    "\n",
    "A commonly known fact about volatility is that it tends to be autocorrelated. We can use this property to implement a naive model that just \"predicts\" realized volatility by using whatever the realized volatility was in the initial 10 minutes.\n",
    "\n",
    "Let's calculate the past realized volatility across the training set to see how predictive a single naive signal can be."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering with the Data from the Training Data Set provided\r\n",
    "\r\n",
    "We will first get the folder names of all the stock Ids whetre the order booking data and the trading data is stored locally"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import glob\r\n",
    "list_order_book_file_train = glob.glob('c:\\\\optiver-realized-volatility-prediction\\\\book_train.parquet/*')\r\n",
    "list_trade_book_file_train = glob.glob('c:\\\\optiver-realized-volatility-prediction\\\\trade_train.parquet/*')\r\n",
    "#list_order_book_file_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the data is partitioned by stock_id (each stock id is one folder), we try to calculcate realized volatility stock by stock and combine them into one target file. Note that the stock id as the partition column is not present if we load the single file so we will remedy that manually. We will reuse the log return and realized volatility functions defined in the previous session."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def compute_wap(df_stock_book):\r\n",
    "       df_stock_book['bid_price'] = (df_stock_book['bid_price1'] * df_stock_book['bid_size1'] + df_stock_book['bid_price2'] * df_stock_book['bid_size2'])/(df_stock_book['bid_size1'] + df_stock_book['bid_size2'])\r\n",
    "       df_stock_book['bid_size'] = (df_stock_book['bid_size1'] + df_stock_book['bid_size2'])\r\n",
    "\r\n",
    "       df_stock_book['ask_price'] = (df_stock_book['ask_price1'] * df_stock_book['ask_size1'] + df_stock_book['ask_price2'] * df_stock_book['ask_size2'])/(df_stock_book['ask_size1'] + df_stock_book['ask_size2'])\r\n",
    "       df_stock_book['ask_size'] = (df_stock_book['ask_size1'] + df_stock_book['ask_size2'])\r\n",
    "\r\n",
    "       df_stock_book['wap'] = (df_stock_book['bid_price'] * df_stock_book['ask_size'] + df_stock_book['ask_price'] * df_stock_book['bid_size']) / (df_stock_book['bid_size'] +  df_stock_book['ask_size'])\r\n",
    "       return df_stock_book['wap']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Sampling\r\n",
    "\r\n",
    "The actual training set contains 167 million records. SInce we will not have high end computing resources, i have picked up 25% of the training data\r\n",
    "\r\n",
    "Below I am randomly picking 25% of the of the stock data randomly. For the same random 25 stocks that is pciked i am selecting below the corresponding trade data so that they can be merged  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import random\r\n",
    "# # Get current state and store\r\n",
    "# state = random.getstate()\r\n",
    "# # set current state\r\n",
    "# random.setstate(state)\r\n",
    "random.seed(42)\r\n",
    "list_order_book_file_train_25 = random.sample(list_order_book_file_train, 25)\r\n",
    "list_trade_book_file_train_25 = [ l.replace('book','trade') for l in list_order_book_file_train_25]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "!pip install pyarrow"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\programdata\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyarrow) (1.19.1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "pd.io.parquet.get_engine('auto')\r\n",
    "pd.io.parquet.PyArrowImpl()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pandas.io.parquet.PyArrowImpl at 0x21bbe848580>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now for selected 25 Stock Ids we will not parse the parquet file and read the order data and trade data into 2 separate dataframes "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "df_order_book_25 = pd.DataFrame()\r\n",
    "for file in list_order_book_file_train_25:\r\n",
    "     df_stock_book = pd.read_parquet(file)\r\n",
    "     df_stock_book['stock_id'] = file.split('=')[1]\r\n",
    "     df_order_book_25 = pd.concat([df_order_book_25,df_stock_book])\r\n",
    "\r\n",
    "\r\n",
    "df_trade_book_25 = pd.DataFrame()\r\n",
    "for file in list_trade_book_file_train_25:\r\n",
    "     df_stock_book = pd.read_parquet(file)\r\n",
    "     df_stock_book['stock_id'] = file.split('=')[1]\r\n",
    "     df_trade_book_25 = pd.concat([df_trade_book_25,df_stock_book])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "df_order_book_25.stock_id.unique(),df_trade_book_25.stock_id.unique()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array(['68', '111', '100', '81', '2', '16', '13', '114', '96', '110',\n",
       "        '73', '9', '55', '109', '61', '39', '101', '97', '85', '126', '14',\n",
       "        '5', '63', '87', '58'], dtype=object),\n",
       " array(['68', '111', '100', '81', '2', '16', '13', '114', '96', '110',\n",
       "        '73', '9', '55', '109', '61', '39', '101', '97', '85', '126', '14',\n",
       "        '5', '63', '87', '58'], dtype=object))"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "df_order_book_25.shape,df_trade_book_25.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((36889097, 11), (8110555, 6))"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Selected Sample Size\r\n",
    "\r\n",
    "Our selected sample size\r\n",
    "Order data is 36 million\r\n",
    "Trade data is 8 million"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "#new_index\r\n",
    "df_order_book_25.reset_index(inplace=True,drop=True)\r\n",
    "df_trade_book_25.reset_index(inplace=True,drop=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputing\r\n",
    "\r\n",
    "There are several gaps in the Order data and the Trade data\r\n",
    "\r\n",
    "Gap #1 - for each time window there should be ideally 600 records for each stock id. but in reality the data does not have that continuous data. So we have introduce the missing rows for the missing 'seconds in bukcet'\r\n",
    "\r\n",
    "Gap #2 - for all time windows not all stock Ids are traded. But this is tru in reality as well. So we will not fill this\r\n",
    "\r\n",
    "Gap #3 - For all order data there will not be traded data because the BUYER and the SELLER have not come to a single pris where the trade settlement can happen. we can see that Order data is 36 million where as the trade data is 8 million. for this purpose what we will assume the for filling the non traded data\r\n",
    "\r\n",
    "    Size and Count (quanity) of trade will be assumed as zero for the non traded order data\r\n",
    "    Price should not be made zero as this will be misleading. so we will assume the last traded prices as the price until the next trade\r\n",
    "\r\n",
    "For Gap #1 - The data Filling Strategy is as follows\r\n",
    "\r\n",
    "    1. first ensure that the Zero \"seconds in bucket\" record is present and it has no null values. if it has null values then backfill the same from the next available \"seconds in bucket\" for that time_id and stock_id\r\n",
    "    2. Once the \"zeroth second\" is filled now we will forward fill the remaining rows for each group of STOCK_ID AND TIME_ID\r\n",
    "    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will first merge and the Order data and Trade data into one data frame and start the IMPUTING process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "df_order_trade_merged = pd.merge(\r\n",
    "    df_order_book_25,\r\n",
    "    df_trade_book_25,\r\n",
    "    how=\"outer\",\r\n",
    "    on=['time_id','stock_id','seconds_in_bucket'],\r\n",
    "    sort=True,\r\n",
    "    suffixes=(\"_x\", \"_y\"),\r\n",
    "    copy=True,\r\n",
    "    indicator=False,\r\n",
    "    validate=\"m:m\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "df_order_trade_merged.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(36889098, 14)"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 36 million plus records in the marged data. We have used Outer join so that we get all of the Order data and Trade data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "#25 Stocks\r\n",
    "df_order_trade_merged.stock_id.unique()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['100', '101', '109', '110', '111', '114', '126', '13', '14', '16',\n",
       "       '2', '39', '5', '55', '58', '61', '63', '68', '73', '81', '85',\n",
       "       '87', '9', '96', '97'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will convert Stock_id from object data type to Int8 data type for optimal memory consumption"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "df_order_trade_merged['stock_id'] = df_order_trade_merged['stock_id'].astype('int8')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "we wil create a new index sequence that will reset the \"seconds_in_bucket\" "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "new_index = pd.Index(np.arange(0,600), name=\"seconds_in_bucket\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the purpose of imputation and to introduce missing rows, we will first group the data by Time_id and stock_id"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "grouped = df_order_trade_merged.groupby(['time_id','stock_id'])\r\n",
    "#df_order_book_50_idxed.index.levels[2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "grouped.count()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>order_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
       "      <th>2</th>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">32767</th>\n",
       "      <th>109</th>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>157</td>\n",
       "      <td>157</td>\n",
       "      <td>157</td>\n",
       "      <td>157</td>\n",
       "      <td>157</td>\n",
       "      <td>157</td>\n",
       "      <td>157</td>\n",
       "      <td>157</td>\n",
       "      <td>157</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>572</td>\n",
       "      <td>572</td>\n",
       "      <td>572</td>\n",
       "      <td>572</td>\n",
       "      <td>572</td>\n",
       "      <td>572</td>\n",
       "      <td>572</td>\n",
       "      <td>572</td>\n",
       "      <td>572</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95748 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  seconds_in_bucket  bid_price1  ask_price1  bid_price2  \\\n",
       "time_id stock_id                                                          \n",
       "5       2                       583         583         583         583   \n",
       "        5                       250         250         250         250   \n",
       "        9                       265         265         265         265   \n",
       "        13                      502         502         502         502   \n",
       "        14                      591         591         591         591   \n",
       "...                             ...         ...         ...         ...   \n",
       "32767   109                     257         257         257         257   \n",
       "        110                     157         157         157         157   \n",
       "        111                     572         572         572         572   \n",
       "        114                     277         277         277         277   \n",
       "        126                     217         217         217         217   \n",
       "\n",
       "                  ask_price2  bid_size1  ask_size1  bid_size2  ask_size2  \\\n",
       "time_id stock_id                                                           \n",
       "5       2                583        583        583        583        583   \n",
       "        5                250        250        250        250        250   \n",
       "        9                265        265        265        265        265   \n",
       "        13               502        502        502        502        502   \n",
       "        14               591        591        591        591        591   \n",
       "...                      ...        ...        ...        ...        ...   \n",
       "32767   109              257        257        257        257        257   \n",
       "        110              157        157        157        157        157   \n",
       "        111              572        572        572        572        572   \n",
       "        114              277        277        277        277        277   \n",
       "        126              217        217        217        217        217   \n",
       "\n",
       "                  price  size  order_count  \n",
       "time_id stock_id                            \n",
       "5       2           159   159          159  \n",
       "        5            36    36           36  \n",
       "        9            40    40           40  \n",
       "        13           74    74           74  \n",
       "        14          276   276          276  \n",
       "...                 ...   ...          ...  \n",
       "32767   109          39    39           39  \n",
       "        110          20    20           20  \n",
       "        111          80    80           80  \n",
       "        114          52    52           52  \n",
       "        126          36    36           36  \n",
       "\n",
       "[95748 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now create a new dataframe with the grouped object above which has Time_Id and Stocck_id as the Index Key....we will intoduce \"seconds_in_bucket\" as another index. While reindexing with the index sequence all missing rows of \"seconds_in_bukcet\"  will be created with NaN values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "s2=grouped.apply(lambda x : x.set_index('seconds_in_bucket').reindex(new_index))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After creating the Index we will drop the following as normal columns which will be left behind adter reindexing above"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "s2.drop(columns=['time_id','stock_id'],inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "s2.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 57448800 entries, (5, 2, 0) to (32767, 126, 599)\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   bid_price1   float32\n",
      " 1   ask_price1   float32\n",
      " 2   bid_price2   float32\n",
      " 3   ask_price2   float32\n",
      " 4   bid_size1    float64\n",
      " 5   ask_size1    float64\n",
      " 6   bid_size2    float64\n",
      " 7   ask_size2    float64\n",
      " 8   price        float32\n",
      " 9   size         float64\n",
      " 10  order_count  float64\n",
      "dtypes: float32(5), float64(6)\n",
      "memory usage: 3.9 GB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After imputing above with missing rows for \"seconds_in_bucket\" we now have a total of 56 million records. \r\n",
    "\r\n",
    "We now have a multi Index of time_id, stock_id and seconds_in_bucket"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "print(s2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(57448800, 11)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "print(s2.index)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MultiIndex([(    5,   2,   0),\n",
      "            (    5,   2,   1),\n",
      "            (    5,   2,   2),\n",
      "            (    5,   2,   3),\n",
      "            (    5,   2,   4),\n",
      "            (    5,   2,   5),\n",
      "            (    5,   2,   6),\n",
      "            (    5,   2,   7),\n",
      "            (    5,   2,   8),\n",
      "            (    5,   2,   9),\n",
      "            ...\n",
      "            (32767, 126, 590),\n",
      "            (32767, 126, 591),\n",
      "            (32767, 126, 592),\n",
      "            (32767, 126, 593),\n",
      "            (32767, 126, 594),\n",
      "            (32767, 126, 595),\n",
      "            (32767, 126, 596),\n",
      "            (32767, 126, 597),\n",
      "            (32767, 126, 598),\n",
      "            (32767, 126, 599)],\n",
      "           names=['time_id', 'stock_id', 'seconds_in_bucket'], length=57448800)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "s2.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>order_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>1.000607</td>\n",
       "      <td>1.000769</td>\n",
       "      <td>1.000526</td>\n",
       "      <td>1.00085</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000688</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000526</td>\n",
       "      <td>1.000769</td>\n",
       "      <td>1.000445</td>\n",
       "      <td>1.00085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000526</td>\n",
       "      <td>1.000769</td>\n",
       "      <td>1.000364</td>\n",
       "      <td>1.00085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000607</td>\n",
       "      <td>1.000769</td>\n",
       "      <td>1.000526</td>\n",
       "      <td>1.00085</td>\n",
       "      <td>100.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000526</td>\n",
       "      <td>1.000769</td>\n",
       "      <td>1.000445</td>\n",
       "      <td>1.00085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    bid_price1  ask_price1  bid_price2  \\\n",
       "time_id stock_id seconds_in_bucket                                       \n",
       "5       2        0                    1.000607    1.000769    1.000526   \n",
       "                 1                    1.000526    1.000769    1.000445   \n",
       "                 2                    1.000526    1.000769    1.000364   \n",
       "                 3                    1.000607    1.000769    1.000526   \n",
       "                 4                    1.000526    1.000769    1.000445   \n",
       "\n",
       "                                    ask_price2  bid_size1  ask_size1  \\\n",
       "time_id stock_id seconds_in_bucket                                     \n",
       "5       2        0                     1.00085      100.0      200.0   \n",
       "                 1                     1.00085        1.0      200.0   \n",
       "                 2                     1.00085        1.0      300.0   \n",
       "                 3                     1.00085      100.0      300.0   \n",
       "                 4                     1.00085        1.0      300.0   \n",
       "\n",
       "                                    bid_size2  ask_size2     price   size  \\\n",
       "time_id stock_id seconds_in_bucket                                          \n",
       "5       2        0                        1.0        1.0  1.000688  101.0   \n",
       "                 1                      100.0      101.0       NaN    NaN   \n",
       "                 2                      112.0      101.0       NaN    NaN   \n",
       "                 3                        1.0        1.0       NaN    NaN   \n",
       "                 4                      300.0      101.0       NaN    NaN   \n",
       "\n",
       "                                    order_count  \n",
       "time_id stock_id seconds_in_bucket               \n",
       "5       2        0                          2.0  \n",
       "                 1                          NaN  \n",
       "                 2                          NaN  \n",
       "                 3                          NaN  \n",
       "                 4                          NaN  "
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is the amount of null values we have after introducing the missing \"seconds\" records"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "s2.isnull().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "bid_price1     20559703\n",
       "ask_price1     20559703\n",
       "bid_price2     20559703\n",
       "ask_price2     20559703\n",
       "bid_size1      20559703\n",
       "ask_size1      20559703\n",
       "bid_size2      20559703\n",
       "ask_size2      20559703\n",
       "price          49338245\n",
       "size           49338245\n",
       "order_count    49338245\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As explained already the imputing strategy we will be filling zero values for the trade data atrributes such as 'size' and 'order count'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "s2[['size','order_count']] = s2[['size','order_count']].fillna(0) \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "s2.isna().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "bid_price1     20559703\n",
       "ask_price1     20559703\n",
       "bid_price2     20559703\n",
       "ask_price2     20559703\n",
       "bid_size1      20559703\n",
       "ask_size1      20559703\n",
       "bid_size2      20559703\n",
       "ask_size2      20559703\n",
       "price          49338245\n",
       "size                  0\n",
       "order_count           0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we will know for each time_id and stock_id which zero seconds record is having null values in their attributes\r\n",
    "\r\n",
    "Only price attribute in trading data is having null values in zero seconds record. Size and Count are already filled with zeros.\r\n",
    "\r\n",
    "In the order booking data all the 8 attributes have values with no Nulls"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "s2.loc(axis=0)[:,:,0].isna().sum()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "bid_price1         0\n",
       "ask_price1         0\n",
       "bid_price2         0\n",
       "ask_price2         0\n",
       "bid_size1          0\n",
       "ask_size1          0\n",
       "bid_size2          0\n",
       "ask_size2          0\n",
       "price          75233\n",
       "size               0\n",
       "order_count        0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "s2.loc(axis=0)[:,:,0][s2.loc(axis=0)[:,:,0]['price'].isna()]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>order_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <td>0.999749</td>\n",
       "      <td>1.001348</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>1.001361</td>\n",
       "      <td>308.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <th>0</th>\n",
       "      <td>1.003797</td>\n",
       "      <td>1.005181</td>\n",
       "      <td>1.003755</td>\n",
       "      <td>1.005223</td>\n",
       "      <td>333.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <th>0</th>\n",
       "      <td>1.000643</td>\n",
       "      <td>1.000710</td>\n",
       "      <td>1.000575</td>\n",
       "      <td>1.000778</td>\n",
       "      <td>53.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>0</th>\n",
       "      <td>1.001464</td>\n",
       "      <td>1.002419</td>\n",
       "      <td>1.001401</td>\n",
       "      <td>1.002547</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <th>0</th>\n",
       "      <td>1.000377</td>\n",
       "      <td>1.000503</td>\n",
       "      <td>1.000314</td>\n",
       "      <td>1.000566</td>\n",
       "      <td>37.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">32767</th>\n",
       "      <th>96</th>\n",
       "      <th>0</th>\n",
       "      <td>0.999584</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>0.999563</td>\n",
       "      <td>0.999970</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <th>0</th>\n",
       "      <td>1.000047</td>\n",
       "      <td>1.000142</td>\n",
       "      <td>0.999953</td>\n",
       "      <td>1.000237</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <th>0</th>\n",
       "      <td>0.999975</td>\n",
       "      <td>1.000370</td>\n",
       "      <td>0.999926</td>\n",
       "      <td>1.000420</td>\n",
       "      <td>100.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <th>0</th>\n",
       "      <td>0.999917</td>\n",
       "      <td>1.000050</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>1.000083</td>\n",
       "      <td>100.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <th>0</th>\n",
       "      <td>0.999949</td>\n",
       "      <td>1.000514</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>1.000617</td>\n",
       "      <td>202.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75233 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    bid_price1  ask_price1  bid_price2  \\\n",
       "time_id stock_id seconds_in_bucket                                       \n",
       "5       5        0                    0.999749    1.001348    0.999655   \n",
       "        9        0                    1.003797    1.005181    1.003755   \n",
       "        14       0                    1.000643    1.000710    1.000575   \n",
       "        16       0                    1.001464    1.002419    1.001401   \n",
       "        39       0                    1.000377    1.000503    1.000314   \n",
       "...                                        ...         ...         ...   \n",
       "32767   96       0                    0.999584    0.999949    0.999563   \n",
       "        101      0                    1.000047    1.000142    0.999953   \n",
       "        109      0                    0.999975    1.000370    0.999926   \n",
       "        114      0                    0.999917    1.000050    0.999850   \n",
       "        126      0                    0.999949    1.000514    0.999846   \n",
       "\n",
       "                                    ask_price2  bid_size1  ask_size1  \\\n",
       "time_id stock_id seconds_in_bucket                                     \n",
       "5       5        0                    1.001361      308.0        5.0   \n",
       "        9        0                    1.005223      333.0      124.0   \n",
       "        14       0                    1.000778       53.0      100.0   \n",
       "        16       0                    1.002547        7.0        1.0   \n",
       "        39       0                    1.000566       37.0      114.0   \n",
       "...                                        ...        ...        ...   \n",
       "32767   96       0                    0.999970      100.0      100.0   \n",
       "        101      0                    1.000237       56.0      100.0   \n",
       "        109      0                    1.000420      100.0      300.0   \n",
       "        114      0                    1.000083      100.0      160.0   \n",
       "        126      0                    1.000617      202.0      276.0   \n",
       "\n",
       "                                    bid_size2  ask_size2  price  size  \\\n",
       "time_id stock_id seconds_in_bucket                                      \n",
       "5       5        0                       11.0       11.0    NaN   0.0   \n",
       "        9        0                      100.0       41.0    NaN   0.0   \n",
       "        14       0                      257.0      201.0    NaN   0.0   \n",
       "        16       0                        9.0       20.0    NaN   0.0   \n",
       "        39       0                      312.0        2.0    NaN   0.0   \n",
       "...                                       ...        ...    ...   ...   \n",
       "32767   96       0                      100.0      100.0    NaN   0.0   \n",
       "        101      0                      189.0      125.0    NaN   0.0   \n",
       "        109      0                       71.0      100.0    NaN   0.0   \n",
       "        114      0                        1.0      100.0    NaN   0.0   \n",
       "        126      0                       20.0        2.0    NaN   0.0   \n",
       "\n",
       "                                    order_count  \n",
       "time_id stock_id seconds_in_bucket               \n",
       "5       5        0                          0.0  \n",
       "        9        0                          0.0  \n",
       "        14       0                          0.0  \n",
       "        16       0                          0.0  \n",
       "        39       0                          0.0  \n",
       "...                                         ...  \n",
       "32767   96       0                          0.0  \n",
       "        101      0                          0.0  \n",
       "        109      0                          0.0  \n",
       "        114      0                          0.0  \n",
       "        126      0                          0.0  \n",
       "\n",
       "[75233 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "for those zero seconds records we, as explained already, we need to do backfill first. for that we need to first find the first available values for each of the time_id and stock_id gorup. Then fill that in the main dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "bf = s2.groupby(['time_id','stock_id'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "bf['price'].first()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "time_id  stock_id\n",
       "5        2           1.000688\n",
       "         5           1.001091\n",
       "         9           1.005213\n",
       "         13          1.000129\n",
       "         14          1.000811\n",
       "                       ...   \n",
       "32767    109         0.999982\n",
       "         110         0.999921\n",
       "         111         1.000101\n",
       "         114         0.999952\n",
       "         126         1.000481\n",
       "Name: price, Length: 95748, dtype: float32"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "s2.loc[s2.index.get_level_values(2)==0,'price'] = bf['price'].first()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "s2.isna().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "bid_price1     20559703\n",
       "ask_price1     20559703\n",
       "bid_price2     20559703\n",
       "ask_price2     20559703\n",
       "bid_size1      20559703\n",
       "ask_size1      20559703\n",
       "bid_size2      20559703\n",
       "ask_size2      20559703\n",
       "price          49263012\n",
       "size                  0\n",
       "order_count           0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Earlier we had 49338245 records where we had price enpty. Now after filling the zero seconds records along with backfilling from the next available value, the number of records which has null values for price has reduced to 49263012"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "s2.loc(axis=0)[:,:,0].isna().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "bid_price1     0\n",
       "ask_price1     0\n",
       "bid_price2     0\n",
       "ask_price2     0\n",
       "bid_size1      0\n",
       "ask_size1      0\n",
       "bid_size2      0\n",
       "ask_size2      0\n",
       "price          0\n",
       "size           0\n",
       "order_count    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have zero seconds record will no null values. By ensuring this, no values will get propogated from one group of time_id and stock_id to another as the first record in each group (which is zero second record) is now having no null values\r\n",
    "\r\n",
    "Next we will forward fill the values for empty rows with previously available rows for each time_id and stock_id"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "#s2.loc[:,:,1:] = s2.loc[:,:,1:].fillna(method='ffill')\r\n",
    "#s2.loc[:,:,1:].fillna(method='ffill',inplace=True)\r\n",
    "s2.fillna(method='ffill',inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "s2.isna().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "bid_price1     0\n",
       "ask_price1     0\n",
       "bid_price2     0\n",
       "ask_price2     0\n",
       "bid_size1      0\n",
       "ask_size1      0\n",
       "bid_size2      0\n",
       "ask_size2      0\n",
       "price          0\n",
       "size           0\n",
       "order_count    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again for the purpose of svvaing the memory as we are delaing with huse datasets, we will reset the data type for each attribute to the minimum possible"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "s2['order_count'] = s2['order_count'].astype('int16')\r\n",
    "s2[['bid_size1','ask_size1','bid_size2','ask_size2','size']] = s2[['bid_size1','ask_size1','bid_size2','ask_size2','size']].astype('int32')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "s2.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 57448800 entries, (5, 2, 0) to (32767, 126, 599)\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   bid_price1   float32\n",
      " 1   ask_price1   float32\n",
      " 2   bid_price2   float32\n",
      " 3   ask_price2   float32\n",
      " 4   bid_size1    int32  \n",
      " 5   ask_size1    int32  \n",
      " 6   bid_size2    int32  \n",
      " 7   ask_size2    int32  \n",
      " 8   price        float32\n",
      " 9   size         int32  \n",
      " 10  order_count  int16  \n",
      "dtypes: float32(5), int16(1), int32(5)\n",
      "memory usage: 2.5 GB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we will compute the Order price as a weighted aveare of the ask price, askisize, bid price, bis size as define above already with the formula. \r\n",
    "\r\n",
    "since we have two ask and bid details for each stock and time winodws, we will first take the average of the bid/ask price and the total bid/ask quanity. we will use the same into the WAP formula given below\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data?cellId=18&cellIds=18&kernelSessionId=67183666"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "import IPython\r\n",
    "url = 'https://www.kaggle.com/embed/jiashenliu/introduction-to-financial-concepts-and-data?cellId=18&cellIds=18&kernelSessionId=67183666'\r\n",
    "\r\n",
    "#url = 'https://www.optiver.com/wp-content/uploads/2021/05/DataBucketing.png'\r\n",
    "\r\n",
    "iframe = '<iframe src=' + url + ' width=700 height=350></iframe>'\r\n",
    "\r\n",
    "IPython.display.HTML(iframe)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<iframe src=https://www.kaggle.com/embed/jiashenliu/introduction-to-financial-concepts-and-data?cellId=18&cellIds=18&kernelSessionId=67183666 width=700 height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "s2['order_wap'] = (((s2['bid_price1'] * s2['bid_size1'] + s2['bid_price2'] * s2['bid_size2'])/(s2['bid_size1'] + s2['bid_size2']) * (s2['ask_size1'] + s2['ask_size2'])) + ((s2['ask_price1'] * s2['ask_size1'] + s2['ask_price2'] * s2['ask_size2'])/(s2['ask_size1'] + s2['ask_size2'])) * (s2['bid_size1'] + s2['bid_size2'])) / ((s2['bid_size1'] + s2['bid_size2']) +  (s2['ask_size1'] + s2['ask_size2']))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "s2['trade_wap'] = s2.groupby(['time_id','stock_id'])['price'].apply(lambda x: x - x.mean())\r\n",
    "#s2['trade_wap_sq'] = s2.groupby(['time_id','stock_id'])['price'].apply(lambda x: x - x.mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "pd.options.display.float_format = '{:,.16f}'.format"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "def log_return(list_stock_prices):\r\n",
    "    return np.log(list_stock_prices).diff()\r\n",
    "\r\n",
    "def trade_volatility(trade_wap_prices):\r\n",
    "    return np.sqrt(np.sum(trade_wap_prices**2)/len(trade_wap_prices))\r\n",
    "\r\n",
    "def order_volatility(series_log_return):\r\n",
    "    return np.sqrt(np.sum(series_log_return**2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "s2['log_return'] = s2.groupby(['time_id','stock_id'])['order_wap'].apply(log_return)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "s2['log_return'].isna().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "95748"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "#remove the rows with null values of log return\r\n",
    "#s2[~s2['log_return'].isnull()]['log_return']\r\n",
    "df_realized_vol_per_stock =  pd.DataFrame(s2[~s2['log_return'].isnull()]['log_return'].groupby(['time_id','stock_id']).agg(order_volatility))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "df_realized_vol_per_stock.isna().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "log_return    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "df_realized_vol_per_stock"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>log_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
       "      <th>2</th>\n",
       "      <td>0.0018731349846348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0046019894070923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0046338266693056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0026797724422067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0023218726273626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">32767</th>\n",
       "      <th>109</th>\n",
       "      <td>0.0014521904522553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.0029210662469268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.0012556677684188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.0018005028832704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.0019010640680790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95748 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         log_return\n",
       "time_id stock_id                   \n",
       "5       2        0.0018731349846348\n",
       "        5        0.0046019894070923\n",
       "        9        0.0046338266693056\n",
       "        13       0.0026797724422067\n",
       "        14       0.0023218726273626\n",
       "...                             ...\n",
       "32767   109      0.0014521904522553\n",
       "        110      0.0029210662469268\n",
       "        111      0.0012556677684188\n",
       "        114      0.0018005028832704\n",
       "        126      0.0019010640680790\n",
       "\n",
       "[95748 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "#df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock.apply(lambda x: fff(x.index.get_level_values(1),x.index.get_level_values(0)), axis=1 )\r\n",
    "#df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock.apply(lambda x: x.index.get_level_values(1)+x.index.get_level_values(0))\r\n",
    "df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock.apply(lambda x: x.index.get_level_values(1).astype(str) + \"-\" + x.index.get_level_values(0).astype(str))\r\n",
    "\r\n",
    "#f'{stock_id}-{x}'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "df_realized_vol_per_stock['trade_return'] = pd.DataFrame(s2['trade_wap'].groupby(['time_id','stock_id']).agg(trade_volatility))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "df_realized_vol_per_stock"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>log_return</th>\n",
       "      <th>row_id</th>\n",
       "      <th>trade_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
       "      <th>2</th>\n",
       "      <td>0.0018731349846348</td>\n",
       "      <td>2-5</td>\n",
       "      <td>0.0005385612692736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0046019894070923</td>\n",
       "      <td>5-5</td>\n",
       "      <td>0.0011968942817501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0046338266693056</td>\n",
       "      <td>9-5</td>\n",
       "      <td>0.0019693513212417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0026797724422067</td>\n",
       "      <td>13-5</td>\n",
       "      <td>0.0004783151735477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0023218726273626</td>\n",
       "      <td>14-5</td>\n",
       "      <td>0.0003629519568591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">32767</th>\n",
       "      <th>109</th>\n",
       "      <td>0.0014521904522553</td>\n",
       "      <td>109-32767</td>\n",
       "      <td>0.0003046170126018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.0029210662469268</td>\n",
       "      <td>110-32767</td>\n",
       "      <td>0.0001787947496083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.0012556677684188</td>\n",
       "      <td>111-32767</td>\n",
       "      <td>0.0003218932480909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.0018005028832704</td>\n",
       "      <td>114-32767</td>\n",
       "      <td>0.0005063665417761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.0019010640680790</td>\n",
       "      <td>126-32767</td>\n",
       "      <td>0.0003025906464201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95748 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         log_return     row_id       trade_return\n",
       "time_id stock_id                                                 \n",
       "5       2        0.0018731349846348        2-5 0.0005385612692736\n",
       "        5        0.0046019894070923        5-5 0.0011968942817501\n",
       "        9        0.0046338266693056        9-5 0.0019693513212417\n",
       "        13       0.0026797724422067       13-5 0.0004783151735477\n",
       "        14       0.0023218726273626       14-5 0.0003629519568591\n",
       "...                             ...        ...                ...\n",
       "32767   109      0.0014521904522553  109-32767 0.0003046170126018\n",
       "        110      0.0029210662469268  110-32767 0.0001787947496083\n",
       "        111      0.0012556677684188  111-32767 0.0003218932480909\n",
       "        114      0.0018005028832704  114-32767 0.0005063665417761\n",
       "        126      0.0019010640680790  126-32767 0.0003025906464201\n",
       "\n",
       "[95748 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# def past_realized_volatility_per_stock(list_file,prediction_column_name):\r\n",
    "#     df_past_realized = pd.DataFrame()\r\n",
    "#     for file in list_file:\r\n",
    "#         df_past_realized = pd.concat([df_past_realized,\r\n",
    "#                                      realized_volatility_per_time_id(file,prediction_column_name)])\r\n",
    "#     return df_past_realized\r\n",
    "# df_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\r\n",
    "#                                                            prediction_column_name='pred')"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's join the output dataframe with train.csv to see the performance of the naive prediction on training set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\r\n",
    "train = train[['row_id','target']]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will evaluate the naive prediction result by two metrics: RMSPE and R squared."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "df_joined = train.merge(df_realized_vol_per_stock, on = ['row_id'], how = 'inner')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "df_joined"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "      <th>log_return</th>\n",
       "      <th>trade_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-5</td>\n",
       "      <td>0.0018475460000000</td>\n",
       "      <td>0.0018731349846348</td>\n",
       "      <td>0.0005385612692736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-11</td>\n",
       "      <td>0.0008059180000000</td>\n",
       "      <td>0.0008150119101629</td>\n",
       "      <td>0.0003309211217708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-16</td>\n",
       "      <td>0.0015811100000000</td>\n",
       "      <td>0.0012938779545948</td>\n",
       "      <td>0.0002343745260598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-31</td>\n",
       "      <td>0.0015986860000000</td>\n",
       "      <td>0.0015226512914523</td>\n",
       "      <td>0.0005046062067635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-62</td>\n",
       "      <td>0.0015032210000000</td>\n",
       "      <td>0.0012423946755007</td>\n",
       "      <td>0.0002491856398152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95743</th>\n",
       "      <td>126-32751</td>\n",
       "      <td>0.0034606180000000</td>\n",
       "      <td>0.0036778114736080</td>\n",
       "      <td>0.0004475510012079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95744</th>\n",
       "      <td>126-32753</td>\n",
       "      <td>0.0031126940000000</td>\n",
       "      <td>0.0035839092452079</td>\n",
       "      <td>0.0011539259769618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95745</th>\n",
       "      <td>126-32758</td>\n",
       "      <td>0.0040696760000000</td>\n",
       "      <td>0.0033929920755327</td>\n",
       "      <td>0.0004800538980768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95746</th>\n",
       "      <td>126-32763</td>\n",
       "      <td>0.0033567180000000</td>\n",
       "      <td>0.0031998809427023</td>\n",
       "      <td>0.0003984458293942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95747</th>\n",
       "      <td>126-32767</td>\n",
       "      <td>0.0020903640000000</td>\n",
       "      <td>0.0019010640680790</td>\n",
       "      <td>0.0003025906464201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95748 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          row_id             target         log_return       trade_return\n",
       "0            2-5 0.0018475460000000 0.0018731349846348 0.0005385612692736\n",
       "1           2-11 0.0008059180000000 0.0008150119101629 0.0003309211217708\n",
       "2           2-16 0.0015811100000000 0.0012938779545948 0.0002343745260598\n",
       "3           2-31 0.0015986860000000 0.0015226512914523 0.0005046062067635\n",
       "4           2-62 0.0015032210000000 0.0012423946755007 0.0002491856398152\n",
       "...          ...                ...                ...                ...\n",
       "95743  126-32751 0.0034606180000000 0.0036778114736080 0.0004475510012079\n",
       "95744  126-32753 0.0031126940000000 0.0035839092452079 0.0011539259769618\n",
       "95745  126-32758 0.0040696760000000 0.0033929920755327 0.0004800538980768\n",
       "95746  126-32763 0.0033567180000000 0.0031998809427023 0.0003984458293942\n",
       "95747  126-32767 0.0020903640000000 0.0019010640680790 0.0003025906464201\n",
       "\n",
       "[95748 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\r\n",
    "def rmspe(y_true, y_pred):\r\n",
    "#    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))) * 100\r\n",
    "    return mean_squared_error(y_true,y_pred,squared=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "# R2 = round(r2_score(y_true = df_joined['target'], y_pred = df_joined['log_return']),3)\r\n",
    "# RMSPE = round(rmspe(y_true = df_joined['target'], y_pred = df_joined['log_return']),3)\r\n",
    "# print(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}%')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "# R2 = round(r2_score(y_true = df_joined['target'], y_pred = df_joined['log_return'] + np.sqrt(df_joined['trade_return'])),3)\r\n",
    "# RMSPE = round(rmspe(y_true = df_joined['target'], y_pred = df_joined['log_return'] + np.sqrt(df_joined['trade_return'])),3)\r\n",
    "# print(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}%')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "jovian.commit(filename=\"my-zero-to-gbm-proj-assign.ipynb\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'jovian' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16228/1433647410.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjovian\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"my-zero-to-gbm-proj-assign.ipynb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'jovian' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Machine Learning\r\n",
    "\r\n",
    "Now we will start applying ML techniques to predict the volataility of the next 10 minutes window for each time-id/stock-id based on the order book volatility and trade volatility\r\n",
    "\r\n",
    "we will learn the hyper parameters givne using the training targets for the same\r\n",
    "\r\n",
    "we will use 2 different models to do the same"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graident Bossting\r\n",
    "\r\n",
    "We're now ready to train our gradient boosting machine (GBM) model. Here's how a GBM model works:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "from xgboost import XGBRegressor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Intel Extension for Scikit-learn\r\n",
    "\r\n",
    "Intel(R) Extension for Scikit-learn* dynamically patches scikit-learn estimators to use Intel(R) oneAPI Data Analytics Library as the underlying solver, while getting the same solution faster.\r\n",
    "\r\n",
    "To install these Intel-optimized packages for scikit-learn on Windows, Mac, and Linux x86_64, simply:\r\n",
    "\r\n",
    "conda install scikit-learn-intelex\r\n",
    "\r\n",
    "Once installed, there are two ways in which you can enable the replacement patching functionality for scikit-learn. You can enable it when you run your application:\r\n",
    "\r\n",
    "python -m sklearnex my_application.py\r\n",
    "\r\n",
    "Or you can explicitly enable the patching in your code:\r\n",
    "\r\n",
    "from sklearnex import patch_sklearn\r\n",
    "\r\n",
    "patch_sklearn()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "from sklearnex import patch_sklearn\r\n",
    "patch_sklearn()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross Validation\r\n",
    "\r\n",
    " create a validation set before training our XGBoost model. We'll use a different validation strategy this time, called <b> ShuffleSplit </b> cross validation (source):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "from sklearn.model_selection import ShuffleSplit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "ss = ShuffleSplit(n_splits = 5, test_size = 0.25, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define a helper function train_and_evaluate which trains a model the given parameters and returns the trained model, training error and validation error."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "def train_and_evaluate(X_train, train_targets, X_val, val_targets, **params):\r\n",
    "    model =XGBRegressor(random_state=11111,n_jobs=-1,validate_parameters=True,objective='reg:squarederror',max_depth=None,subsample=0.8,tree_method='approx',verbosity=1, **params)\r\n",
    "    model.fit(X_train, train_targets)\r\n",
    "    train_R2 = round(r2_score(train_targets, model.predict(X_train)),3)\r\n",
    "    train_RMSPE = round(rmspe(train_targets, model.predict(X_train)),3)\r\n",
    "    val_R2 = round(r2_score(val_targets, model.predict(X_val)),3)\r\n",
    "    val_RMSPE = round(rmspe(val_targets, model.predict(X_val),),3)\r\n",
    "\r\n",
    "    # train_rmse = rmse(model.predict(X_train), train_targets)\r\n",
    "    # val_rmse = rmse(model.predict(X_val), val_targets)\r\n",
    "    return model, train_R2, train_RMSPE, val_R2, val_RMSPE\r\n",
    "#n_jobs = -1 means that use all the available threads in that machine where the alogorithm is running "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will train the model for each split data of the ShuffleSplit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "df_joined.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 95748 entries, 0 to 95747\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   row_id        95748 non-null  object \n",
      " 1   target        95748 non-null  float64\n",
      " 2   log_return    95748 non-null  float32\n",
      " 3   trade_return  95748 non-null  float64\n",
      "dtypes: float32(1), float64(2), object(1)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "inputs = df_joined[['log_return', 'trade_return']].copy().astype('float32')\r\n",
    "targets = df_joined['target'].copy().astype('float32')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "inputs.isna().sum(), targets.isna().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(log_return      0\n",
       " trade_return    0\n",
       " dtype: int64,\n",
       " 0)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "inputs.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 95748 entries, 0 to 95747\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   log_return    95748 non-null  float32\n",
      " 1   trade_return  95748 non-null  float32\n",
      "dtypes: float32(2)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "targets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       0.0018475459655747\n",
       "1       0.0008059180108830\n",
       "2       0.0015811099437997\n",
       "3       0.0015986859798431\n",
       "4       0.0015032209921628\n",
       "               ...        \n",
       "95743   0.0034606179688126\n",
       "95744   0.0031126940157264\n",
       "95745   0.0040696761570871\n",
       "95746   0.0033567179925740\n",
       "95747   0.0020903639961034\n",
       "Name: target, Length: 95748, dtype: float32"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# models = []\r\n",
    "\r\n",
    "# for train_idxs, val_idxs in ss.split(inputs):\r\n",
    "#     X_train, train_targets = inputs.iloc[train_idxs], targets.iloc[train_idxs]\r\n",
    "#     X_val, val_targets = inputs.iloc[val_idxs], targets.iloc[val_idxs]\r\n",
    "#     model, train_R2, train_RMSPE, val_R2, val_RMSPE = train_and_evaluate(X_train, \r\n",
    "#                                                      train_targets, \r\n",
    "#                                                      X_val, \r\n",
    "#                                                      val_targets, \r\n",
    "#                                                      max_depth=5, \r\n",
    "#                                                      n_estimators=50)\r\n",
    "#     models.append(model)\r\n",
    "#     print('Train R2: {}, Train RMSPE: {}, Validation R2: {}, Validation RMSPE: {}'.format(train_R2, train_RMSPE, val_R2, val_RMSPE))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also define a function to average predictions from the 5 different models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def predict_avg(models, inputs):\r\n",
    "    return np.mean([model.predict(inputs) for model in models], axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# explicitly require this experimental feature\r\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\r\n",
    "# now you can import normally from ensemble\r\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\r\n",
    "from sklearn.experimental import enable_halving_search_cv\r\n",
    "# now you can import normally from model_selection\r\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, HalvingGridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "model =XGBRegressor(random_state=11111,n_jobs=-1,validate_parameters=True,objective='reg:squarederror',max_depth=None,subsample=0.8,tree_method='approx',verbosity=1)\r\n",
    "param_grid = {#\"max_depth\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, None],\r\n",
    "              \"n_estimators\" : list(range(10,51,3)),\r\n",
    "              \"learning_rate\": np.logspace(-3,3,50),\r\n",
    "              \"booster\": ['gbtree', 'gblinear'],\r\n",
    "              # \"gamma\": np.logspace(-1,1,100),\r\n",
    "              # \"subsample\": np.linspace(0.5,1.0,9, endpoint=False),\r\n",
    "              # \"num_parallel_tree\": range(1,6,1),\r\n",
    "              # \"reg_alpha\": np.logspace(-3,3,100),\r\n",
    "              # \"reg_lambda\": np.logspace(-3,3,100)\r\n",
    "            }\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "models = []\r\n",
    "for train_idxs, val_idxs in ss.split(inputs):\r\n",
    "    X_train, train_targets = inputs.iloc[train_idxs], targets.iloc[train_idxs]\r\n",
    "    X_val, val_targets = inputs.iloc[val_idxs], targets.iloc[val_idxs]\r\n",
    "    X_train = np.ascontiguousarray(X_train).reshape(-1,2)\r\n",
    "    train_targets = np.ascontiguousarray(train_targets).reshape(-1,1)\r\n",
    "    X_val = np.ascontiguousarray(X_val).reshape(-1,2)\r\n",
    "    val_targets = np.ascontiguousarray(val_targets).reshape(-1,1)\r\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs=-1,verbose=1).fit(X_train, np.ravel(train_targets))\r\n",
    "    model, train_R2, train_RMSPE, val_R2, val_RMSPE = train_and_evaluate(X_train, \r\n",
    "                                                    np.ravel(train_targets), \r\n",
    "                                                    X_val, \r\n",
    "                                                    np.ravel(val_targets), \r\n",
    "                                                    **grid_search.best_params_)\r\n",
    "    models.append(model)\r\n",
    "    print('Train R2: {}, Train RMSPE: {}, Validation R2: {}, Validation RMSPE: {}'.format(train_R2, train_RMSPE, val_R2, val_RMSPE))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 1400 candidates, totalling 7000 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [-29966.41975708 -29787.07431983 -29608.8023478  ...             nan\n",
      "             nan             nan]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:929: RuntimeWarning: invalid value encountered in subtract\n",
      "  array_stds = np.sqrt(np.average((array -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train R2: 0.765, Train RMSPE: 0.0010000000474974513, Validation R2: 0.753, Validation RMSPE: 0.0010000000474974513\n",
      "Fitting 5 folds for each of 1400 candidates, totalling 7000 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [-29793.15708523 -29614.84646366 -29437.60599766 ...             nan\n",
      "             nan             nan]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:929: RuntimeWarning: invalid value encountered in subtract\n",
      "  array_stds = np.sqrt(np.average((array -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train R2: 0.766, Train RMSPE: 0.0010000000474974513, Validation R2: 0.751, Validation RMSPE: 0.0010000000474974513\n",
      "Fitting 5 folds for each of 1400 candidates, totalling 7000 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [-29694.98924795 -29517.26759526 -29340.61175239 ...             nan\n",
      "             nan             nan]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:929: RuntimeWarning: invalid value encountered in subtract\n",
      "  array_stds = np.sqrt(np.average((array -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train R2: 0.764, Train RMSPE: 0.0010000000474974513, Validation R2: 0.758, Validation RMSPE: 0.0010000000474974513\n",
      "Fitting 5 folds for each of 1400 candidates, totalling 7000 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [-29799.48371971 -29621.13651364 -29443.85748119 ...             nan\n",
      "             nan             nan]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:929: RuntimeWarning: invalid value encountered in subtract\n",
      "  array_stds = np.sqrt(np.average((array -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train R2: 0.763, Train RMSPE: 0.0010000000474974513, Validation R2: 0.759, Validation RMSPE: 0.0010000000474974513\n",
      "Fitting 5 folds for each of 1400 candidates, totalling 7000 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [-29608.23390888 -29431.03288569 -29254.89036622 ...             nan\n",
      "             nan             nan]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:929: RuntimeWarning: invalid value encountered in subtract\n",
      "  array_stds = np.sqrt(np.average((array -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train R2: 0.761, Train RMSPE: 0.0010000000474974513, Validation R2: 0.764, Validation RMSPE: 0.0010000000474974513\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# models = []\r\n",
    "# for train_idxs, val_idxs in ss.split(inputs):\r\n",
    "#     X_train, train_targets = inputs.iloc[train_idxs], targets.iloc[train_idxs]\r\n",
    "#     X_val, val_targets = inputs.iloc[val_idxs], targets.iloc[val_idxs]\r\n",
    "#     X_train = np.ascontiguousarray(X_train).reshape(-1,2)\r\n",
    "#     train_targets = np.ascontiguousarray(train_targets).reshape(-1,1)\r\n",
    "#     X_val = np.ascontiguousarray(X_val).reshape(-1,2)\r\n",
    "#     val_targets = np.ascontiguousarray(val_targets).reshape(-1,1)\r\n",
    "#     reg = LazyRegressor(ignore_warnings=False, random_state=11111, verbose=True)\r\n",
    "#     models, predictions = reg.fit(X_train, X_val, train_targets, val_targets)  # pass all sets\r\n",
    "#     print(models.head(100))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "targets >= 0"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0        True\n",
       "1        True\n",
       "2        True\n",
       "3        True\n",
       "4        True\n",
       "         ... \n",
       "95743    True\n",
       "95744    True\n",
       "95745    True\n",
       "95746    True\n",
       "95747    True\n",
       "Name: target, Length: 95748, dtype: bool"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "model = HistGradientBoostingRegressor(random_state=11111,scoring='neg_root_mean_squared_error',validation_fraction=0.2,verbose=1,loss='poisson')\r\n",
    "param_grid = {\"max_depth\": [10, 20, 30, 40, 50, None],\r\n",
    "              \"max_leaf_nodes\" : [15,31,63,127],\r\n",
    "              \"learning_rate\": np.logspace(-3,3,50),\r\n",
    "              \"min_samples_leaf\": [10,20,30],\r\n",
    "              \"l2_regularization\": [0.0,0.00001,0.0001,0.001,0.01]\r\n",
    "            }\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "def train_and_evaluate(X_train, train_targets, X_val, val_targets, **params):\r\n",
    "    model = HistGradientBoostingRegressor(random_state=11111,scoring='neg_root_mean_squared_error',validation_fraction=0.2,verbose=1,loss='poisson', **params)\r\n",
    "    model.fit(X_train, train_targets)\r\n",
    "    train_R2 = round(r2_score(train_targets, model.predict(X_train)),3)\r\n",
    "    train_RMSPE = round(rmspe(train_targets, model.predict(X_train)),3)\r\n",
    "    val_R2 = round(r2_score(val_targets, model.predict(X_val)),3)\r\n",
    "    val_RMSPE = round(rmspe(val_targets, model.predict(X_val),),3)\r\n",
    "\r\n",
    "    # train_rmse = rmse(model.predict(X_train), train_targets)\r\n",
    "    # val_rmse = rmse(model.predict(X_val), val_targets)\r\n",
    "    return model, train_R2, train_RMSPE, val_R2, val_RMSPE\r\n",
    "#n_jobs = -1 means that use all the available threads in that machine where the alogorithm is running "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "# import sklearn.metrics\r\n",
    "# sorted(sklearn.metrics.SCORERS.keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "models = []\r\n",
    "for train_idxs, val_idxs in ss.split(inputs):\r\n",
    "    X_train, train_targets = inputs.iloc[train_idxs], targets.iloc[train_idxs]\r\n",
    "    X_val, val_targets = inputs.iloc[val_idxs], targets.iloc[val_idxs]\r\n",
    "    X_train = np.ascontiguousarray(X_train).reshape(-1,2)\r\n",
    "    train_targets = np.ascontiguousarray(train_targets).reshape(-1,1)\r\n",
    "    X_val = np.ascontiguousarray(X_val).reshape(-1,2)\r\n",
    "    val_targets = np.ascontiguousarray(val_targets).reshape(-1,1)\r\n",
    "    grid_search = RandomizedSearchCV(model, param_grid, random_state=1111, scoring='r2',verbose=1,return_train_score=True,n_jobs=-1,n_iter=20).fit(X_train,train_targets)\r\n",
    "    model, train_R2, train_RMSPE, val_R2, val_RMSPE = train_and_evaluate(X_train, \r\n",
    "                                                        train_targets, \r\n",
    "                                                        X_val, \r\n",
    "                                                        val_targets, \r\n",
    "                                                        **grid_search.best_params_)\r\n",
    "    models.append(model)\r\n",
    "    print('Train R2: {}, Train RMSPE: {}, Validation R2: {}, Validation RMSPE: {}'.format(train_R2, train_RMSPE, val_R2, val_RMSPE))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binning 0.001 GB of training data: 0.109 s\n",
      "Binning 0.000 GB of validation data: 0.022 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00214, val score: -0.00213, in 0.163s\n",
      "[2/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00180, val score: -0.00179, in 0.109s\n",
      "[3/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00161, val score: -0.00160, in 0.149s\n",
      "[4/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00150, val score: -0.00149, in 0.133s\n",
      "[5/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00144, val score: -0.00143, in 0.134s\n",
      "[6/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00140, val score: -0.00139, in 0.144s\n",
      "[7/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.153s\n",
      "[8/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00138, val score: -0.00137, in 0.150s\n",
      "[9/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00137, val score: -0.00136, in 0.156s\n",
      "[10/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00137, val score: -0.00136, in 0.156s\n",
      "[11/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00136, in 0.160s\n",
      "[12/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00137, val score: -0.00136, in 0.216s\n",
      "[13/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00137, val score: -0.00136, in 0.204s\n",
      "[14/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00137, val score: -0.00136, in 0.239s\n",
      "[15/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00136, val score: -0.00136, in 0.204s\n",
      "[16/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00136, val score: -0.00136, in 0.182s\n",
      "[17/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00136, val score: -0.00136, in 0.183s\n",
      "[18/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00136, val score: -0.00136, in 0.190s\n",
      "[19/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00136, val score: -0.00136, in 0.193s\n",
      "[20/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00136, val score: -0.00136, in 0.200s\n",
      "[21/100] 1 tree, 15 leaves, max depth = 10, train score: -0.00136, val score: -0.00136, in 0.200s\n",
      "[22/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00136, val score: -0.00136, in 0.206s\n",
      "[23/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00136, val score: -0.00136, in 0.203s\n",
      "[24/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00136, val score: -0.00136, in 0.223s\n",
      "[25/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00136, val score: -0.00136, in 0.285s\n",
      "Fit 25 trees in 6.456 s, (375 total leaves)\n",
      "Time spent computing histograms: 0.218s\n",
      "Time spent finding best splits:  0.046s\n",
      "Time spent applying splits:      0.228s\n",
      "Time spent predicting:           0.010s\n",
      "Binning 0.001 GB of training data: 0.113 s\n",
      "Binning 0.000 GB of validation data: 0.029 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00214, val score: -0.00213, in 0.077s\n",
      "[2/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00180, val score: -0.00179, in 0.124s\n",
      "[3/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00161, val score: -0.00160, in 0.115s\n",
      "[4/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00150, val score: -0.00149, in 0.120s\n",
      "[5/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00144, val score: -0.00143, in 0.145s\n",
      "[6/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00140, val score: -0.00139, in 0.130s\n",
      "[7/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.123s\n",
      "[8/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00138, val score: -0.00137, in 0.132s\n",
      "[9/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00137, val score: -0.00136, in 0.152s\n",
      "[10/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00137, val score: -0.00136, in 0.143s\n",
      "[11/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00136, in 0.151s\n",
      "[12/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00137, val score: -0.00136, in 0.145s\n",
      "[13/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00137, val score: -0.00136, in 0.166s\n",
      "[14/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00137, val score: -0.00136, in 0.154s\n",
      "[15/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00136, val score: -0.00136, in 0.162s\n",
      "[16/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00136, val score: -0.00136, in 0.166s\n",
      "[17/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00136, val score: -0.00136, in 0.175s\n",
      "[18/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00136, val score: -0.00136, in 0.173s\n",
      "[19/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00136, val score: -0.00136, in 0.169s\n",
      "[20/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00136, val score: -0.00136, in 0.184s\n",
      "[21/100] 1 tree, 15 leaves, max depth = 10, train score: -0.00136, val score: -0.00136, in 0.196s\n",
      "[22/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00136, val score: -0.00136, in 0.192s\n",
      "[23/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00136, val score: -0.00136, in 0.199s\n",
      "[24/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00136, val score: -0.00136, in 0.204s\n",
      "[25/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00136, val score: -0.00136, in 0.221s\n",
      "Fit 25 trees in 4.344 s, (375 total leaves)\n",
      "Time spent computing histograms: 0.170s\n",
      "Time spent finding best splits:  0.025s\n",
      "Time spent applying splits:      0.237s\n",
      "Time spent predicting:           0.006s\n",
      "Train R2: 0.77, Train RMSPE: 0.001, Validation R2: 0.759, Validation RMSPE: 0.001\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   58.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binning 0.001 GB of training data: 0.115 s\n",
      "Binning 0.000 GB of validation data: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.031 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 31 leaves, max depth = 5, train score: -0.00231, val score: -0.00234, in 0.122s\n",
      "[2/100] 1 tree, 31 leaves, max depth = 6, train score: -0.00198, val score: -0.00202, in 0.166s\n",
      "[3/100] 1 tree, 31 leaves, max depth = 6, train score: -0.00176, val score: -0.00180, in 0.159s\n",
      "[4/100] 1 tree, 31 leaves, max depth = 9, train score: -0.00161, val score: -0.00166, in 0.172s\n",
      "[5/100] 1 tree, 31 leaves, max depth = 7, train score: -0.00152, val score: -0.00156, in 0.186s\n",
      "[6/100] 1 tree, 31 leaves, max depth = 7, train score: -0.00145, val score: -0.00150, in 0.190s\n",
      "[7/100] 1 tree, 31 leaves, max depth = 8, train score: -0.00141, val score: -0.00146, in 0.187s\n",
      "[8/100] 1 tree, 31 leaves, max depth = 9, train score: -0.00139, val score: -0.00143, in 0.198s\n",
      "[9/100] 1 tree, 31 leaves, max depth = 8, train score: -0.00137, val score: -0.00142, in 0.239s\n",
      "[10/100] 1 tree, 31 leaves, max depth = 9, train score: -0.00136, val score: -0.00141, in 0.189s\n",
      "[11/100] 1 tree, 31 leaves, max depth = 9, train score: -0.00135, val score: -0.00140, in 0.198s\n",
      "[12/100] 1 tree, 31 leaves, max depth = 12, train score: -0.00135, val score: -0.00140, in 0.195s\n",
      "[13/100] 1 tree, 31 leaves, max depth = 12, train score: -0.00135, val score: -0.00140, in 0.207s\n",
      "[14/100] 1 tree, 31 leaves, max depth = 15, train score: -0.00134, val score: -0.00140, in 0.213s\n",
      "[15/100] 1 tree, 31 leaves, max depth = 10, train score: -0.00134, val score: -0.00139, in 0.226s\n",
      "[16/100] 1 tree, 31 leaves, max depth = 12, train score: -0.00134, val score: -0.00139, in 0.239s\n",
      "[17/100] 1 tree, 31 leaves, max depth = 11, train score: -0.00134, val score: -0.00139, in 0.246s\n",
      "[18/100] 1 tree, 31 leaves, max depth = 12, train score: -0.00134, val score: -0.00139, in 0.243s\n",
      "[19/100] 1 tree, 31 leaves, max depth = 10, train score: -0.00134, val score: -0.00139, in 0.243s\n",
      "[20/100] 1 tree, 31 leaves, max depth = 10, train score: -0.00134, val score: -0.00139, in 0.259s\n",
      "[21/100] 1 tree, 31 leaves, max depth = 14, train score: -0.00134, val score: -0.00139, in 0.259s\n",
      "[22/100] 1 tree, 31 leaves, max depth = 9, train score: -0.00134, val score: -0.00139, in 0.242s\n",
      "[23/100] 1 tree, 31 leaves, max depth = 13, train score: -0.00134, val score: -0.00139, in 0.268s\n",
      "[24/100] 1 tree, 31 leaves, max depth = 10, train score: -0.00134, val score: -0.00139, in 0.294s\n",
      "[25/100] 1 tree, 31 leaves, max depth = 11, train score: -0.00134, val score: -0.00139, in 0.306s\n",
      "[26/100] 1 tree, 31 leaves, max depth = 9, train score: -0.00134, val score: -0.00139, in 0.313s\n",
      "[27/100] 1 tree, 31 leaves, max depth = 14, train score: -0.00134, val score: -0.00139, in 0.283s\n",
      "Fit 27 trees in 6.478 s, (837 total leaves)\n",
      "Time spent computing histograms: 0.351s\n",
      "Time spent finding best splits:  0.064s\n",
      "Time spent applying splits:      0.317s\n",
      "Time spent predicting:           0.017s\n",
      "Binning 0.001 GB of training data: 0.234 s\n",
      "Binning 0.000 GB of validation data: 0.021 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 31 leaves, max depth = 5, train score: -0.00231, val score: -0.00234, in 0.101s\n",
      "[2/100] 1 tree, 31 leaves, max depth = 6, train score: -0.00198, val score: -0.00202, in 0.140s\n",
      "[3/100] 1 tree, 31 leaves, max depth = 6, train score: -0.00176, val score: -0.00180, in 0.120s\n",
      "[4/100] 1 tree, 31 leaves, max depth = 9, train score: -0.00161, val score: -0.00166, in 0.188s\n",
      "[5/100] 1 tree, 31 leaves, max depth = 7, train score: -0.00152, val score: -0.00156, in 0.140s\n",
      "[6/100] 1 tree, 31 leaves, max depth = 7, train score: -0.00145, val score: -0.00150, in 0.152s\n",
      "[7/100] 1 tree, 31 leaves, max depth = 8, train score: -0.00141, val score: -0.00146, in 0.160s\n",
      "[8/100] 1 tree, 31 leaves, max depth = 9, train score: -0.00139, val score: -0.00143, in 0.331s\n",
      "[9/100] 1 tree, 31 leaves, max depth = 8, train score: -0.00137, val score: -0.00142, in 0.216s\n",
      "[10/100] 1 tree, 31 leaves, max depth = 9, train score: -0.00136, val score: -0.00141, in 0.190s\n",
      "[11/100] 1 tree, 31 leaves, max depth = 9, train score: -0.00135, val score: -0.00140, in 0.214s\n",
      "[12/100] 1 tree, 31 leaves, max depth = 12, train score: -0.00135, val score: -0.00140, in 0.222s\n",
      "[13/100] 1 tree, 31 leaves, max depth = 12, train score: -0.00135, val score: -0.00140, in 0.212s\n",
      "[14/100] 1 tree, 31 leaves, max depth = 15, train score: -0.00134, val score: -0.00140, in 0.218s\n",
      "[15/100] 1 tree, 31 leaves, max depth = 10, train score: -0.00134, val score: -0.00139, in 0.251s\n",
      "[16/100] 1 tree, 31 leaves, max depth = 12, train score: -0.00134, val score: -0.00139, in 0.244s\n",
      "[17/100] 1 tree, 31 leaves, max depth = 11, train score: -0.00134, val score: -0.00139, in 0.249s\n",
      "[18/100] 1 tree, 31 leaves, max depth = 12, train score: -0.00134, val score: -0.00139, in 0.281s\n",
      "[19/100] 1 tree, 31 leaves, max depth = 10, train score: -0.00134, val score: -0.00139, in 0.290s\n",
      "[20/100] 1 tree, 31 leaves, max depth = 10, train score: -0.00134, val score: -0.00139, in 0.299s\n",
      "[21/100] 1 tree, 31 leaves, max depth = 14, train score: -0.00134, val score: -0.00139, in 0.280s\n",
      "[22/100] 1 tree, 31 leaves, max depth = 9, train score: -0.00134, val score: -0.00139, in 0.287s\n",
      "[23/100] 1 tree, 31 leaves, max depth = 13, train score: -0.00134, val score: -0.00139, in 0.287s\n",
      "[24/100] 1 tree, 31 leaves, max depth = 10, train score: -0.00134, val score: -0.00139, in 0.272s\n",
      "[25/100] 1 tree, 31 leaves, max depth = 11, train score: -0.00134, val score: -0.00139, in 0.289s\n",
      "[26/100] 1 tree, 31 leaves, max depth = 9, train score: -0.00134, val score: -0.00139, in 0.254s\n",
      "[27/100] 1 tree, 31 leaves, max depth = 14, train score: -0.00134, val score: -0.00139, in 0.274s\n",
      "Fit 27 trees in 6.672 s, (837 total leaves)\n",
      "Time spent computing histograms: 0.308s\n",
      "Time spent finding best splits:  0.061s\n",
      "Time spent applying splits:      0.346s\n",
      "Time spent predicting:           0.008s\n",
      "Train R2: 0.773, Train RMSPE: 0.001, Validation R2: 0.757, Validation RMSPE: 0.001\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.0min\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binning 0.001 GB of training data: 0.050 s\n",
      "Binning 0.000 GB of validation data: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.019 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00220, val score: -0.00216, in 0.075s\n",
      "[2/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00185, val score: -0.00182, in 0.096s\n",
      "[3/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00166, val score: -0.00162, in 0.073s\n",
      "[4/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00155, val score: -0.00151, in 0.104s\n",
      "[5/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00149, val score: -0.00145, in 0.143s\n",
      "[6/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00145, val score: -0.00141, in 0.206s\n",
      "[7/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00143, val score: -0.00139, in 0.266s\n",
      "[8/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00142, val score: -0.00138, in 0.238s\n",
      "[9/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00141, val score: -0.00137, in 0.135s\n",
      "[10/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00141, val score: -0.00137, in 0.116s\n",
      "[11/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00141, val score: -0.00137, in 0.118s\n",
      "[12/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00141, val score: -0.00137, in 0.126s\n",
      "[13/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00140, val score: -0.00137, in 0.125s\n",
      "[14/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00140, val score: -0.00137, in 0.188s\n",
      "[15/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00140, val score: -0.00137, in 0.169s\n",
      "[16/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00140, val score: -0.00137, in 0.151s\n",
      "[17/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00140, val score: -0.00137, in 0.203s\n",
      "[18/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00140, val score: -0.00137, in 0.214s\n",
      "[19/100] 1 tree, 15 leaves, max depth = 10, train score: -0.00140, val score: -0.00137, in 0.234s\n",
      "[20/100] 1 tree, 15 leaves, max depth = 10, train score: -0.00140, val score: -0.00137, in 0.153s\n",
      "[21/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00140, val score: -0.00137, in 0.140s\n",
      "[22/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00140, val score: -0.00137, in 0.140s\n",
      "[23/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00140, val score: -0.00137, in 0.347s\n",
      "[24/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00140, val score: -0.00137, in 0.183s\n",
      "[25/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00140, val score: -0.00137, in 0.345s\n",
      "[26/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00140, val score: -0.00137, in 0.299s\n",
      "Fit 26 trees in 4.855 s, (390 total leaves)\n",
      "Time spent computing histograms: 0.162s\n",
      "Time spent finding best splits:  0.034s\n",
      "Time spent applying splits:      0.217s\n",
      "Time spent predicting:           0.011s\n",
      "Binning 0.001 GB of training data: 0.094 s\n",
      "Binning 0.000 GB of validation data: 0.024 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00220, val score: -0.00216, in 0.077s\n",
      "[2/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00185, val score: -0.00182, in 0.080s\n",
      "[3/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00166, val score: -0.00162, in 0.082s\n",
      "[4/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00155, val score: -0.00151, in 0.095s\n",
      "[5/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00149, val score: -0.00145, in 0.088s\n",
      "[6/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00145, val score: -0.00141, in 0.091s\n",
      "[7/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00143, val score: -0.00139, in 0.098s\n",
      "[8/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00142, val score: -0.00138, in 0.113s\n",
      "[9/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00141, val score: -0.00137, in 0.099s\n",
      "[10/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00141, val score: -0.00137, in 0.105s\n",
      "[11/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00141, val score: -0.00137, in 0.105s\n",
      "[12/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00141, val score: -0.00137, in 0.109s\n",
      "[13/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00140, val score: -0.00137, in 0.113s\n",
      "[14/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00140, val score: -0.00137, in 0.129s\n",
      "[15/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00140, val score: -0.00137, in 0.215s\n",
      "[16/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00140, val score: -0.00137, in 0.149s\n",
      "[17/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00140, val score: -0.00137, in 0.168s\n",
      "[18/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00140, val score: -0.00137, in 0.141s\n",
      "[19/100] 1 tree, 15 leaves, max depth = 10, train score: -0.00140, val score: -0.00137, in 0.149s\n",
      "[20/100] 1 tree, 15 leaves, max depth = 10, train score: -0.00140, val score: -0.00137, in 0.182s\n",
      "[21/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00140, val score: -0.00137, in 0.223s\n",
      "[22/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00140, val score: -0.00137, in 0.228s\n",
      "[23/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00140, val score: -0.00137, in 0.194s\n",
      "[24/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00140, val score: -0.00137, in 0.148s\n",
      "[25/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00140, val score: -0.00137, in 0.147s\n",
      "[26/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00140, val score: -0.00137, in 0.145s\n",
      "Fit 26 trees in 3.787 s, (390 total leaves)\n",
      "Time spent computing histograms: 0.186s\n",
      "Time spent finding best splits:  0.033s\n",
      "Time spent applying splits:      0.206s\n",
      "Time spent predicting:           0.006s\n",
      "Train R2: 0.769, Train RMSPE: 0.001, Validation R2: 0.764, Validation RMSPE: 0.001\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   45.4s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binning 0.001 GB of training data: 0.078 s\n",
      "Binning 0.000 GB of validation data: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.016 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00214, val score: -0.00216, in 0.121s\n",
      "[2/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00180, val score: -0.00182, in 0.107s\n",
      "[3/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00160, val score: -0.00162, in 0.116s\n",
      "[4/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00150, val score: -0.00152, in 0.116s\n",
      "[5/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00144, val score: -0.00146, in 0.136s\n",
      "[6/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00141, val score: -0.00143, in 0.151s\n",
      "[7/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00139, val score: -0.00141, in 0.156s\n",
      "[8/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00138, val score: -0.00140, in 0.169s\n",
      "[9/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00138, val score: -0.00140, in 0.152s\n",
      "[10/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00138, val score: -0.00140, in 0.139s\n",
      "[11/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00139, in 0.147s\n",
      "[12/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00137, val score: -0.00139, in 0.142s\n",
      "[13/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00137, val score: -0.00139, in 0.147s\n",
      "[14/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00139, in 0.154s\n",
      "[15/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00139, in 0.160s\n",
      "[16/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00139, in 0.158s\n",
      "[17/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00137, val score: -0.00139, in 0.164s\n",
      "[18/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00139, in 0.162s\n",
      "[19/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00137, val score: -0.00139, in 0.188s\n",
      "[20/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00137, val score: -0.00139, in 0.169s\n",
      "[21/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00139, in 0.169s\n",
      "[22/100] 1 tree, 15 leaves, max depth = 10, train score: -0.00137, val score: -0.00139, in 0.181s\n",
      "[23/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00137, val score: -0.00139, in 0.184s\n",
      "Fit 23 trees in 3.743 s, (345 total leaves)\n",
      "Time spent computing histograms: 0.188s\n",
      "Time spent finding best splits:  0.025s\n",
      "Time spent applying splits:      0.291s\n",
      "Time spent predicting:           0.006s\n",
      "Binning 0.001 GB of training data: 0.085 s\n",
      "Binning 0.000 GB of validation data: 0.013 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00214, val score: -0.00216, in 0.072s\n",
      "[2/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00180, val score: -0.00182, in 0.094s\n",
      "[3/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00160, val score: -0.00162, in 0.098s\n",
      "[4/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00150, val score: -0.00152, in 0.110s\n",
      "[5/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00144, val score: -0.00146, in 0.105s\n",
      "[6/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00141, val score: -0.00143, in 0.116s\n",
      "[7/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00139, val score: -0.00141, in 0.115s\n",
      "[8/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00138, val score: -0.00140, in 0.122s\n",
      "[9/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00138, val score: -0.00140, in 0.119s\n",
      "[10/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00138, val score: -0.00140, in 0.128s\n",
      "[11/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00139, in 0.130s\n",
      "[12/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00137, val score: -0.00139, in 0.131s\n",
      "[13/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00137, val score: -0.00139, in 0.138s\n",
      "[14/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00139, in 0.141s\n",
      "[15/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00139, in 0.162s\n",
      "[16/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00139, in 0.194s\n",
      "[17/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00137, val score: -0.00139, in 0.209s\n",
      "[18/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00139, in 0.188s\n",
      "[19/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00137, val score: -0.00139, in 0.150s\n",
      "[20/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00137, val score: -0.00139, in 0.162s\n",
      "[21/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00137, val score: -0.00139, in 0.158s\n",
      "[22/100] 1 tree, 15 leaves, max depth = 10, train score: -0.00137, val score: -0.00139, in 0.170s\n",
      "[23/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00137, val score: -0.00139, in 0.180s\n",
      "Fit 23 trees in 3.485 s, (345 total leaves)\n",
      "Time spent computing histograms: 0.211s\n",
      "Time spent finding best splits:  0.022s\n",
      "Time spent applying splits:      0.225s\n",
      "Time spent predicting:           0.009s\n",
      "Train R2: 0.767, Train RMSPE: 0.001, Validation R2: 0.767, Validation RMSPE: 0.001\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   46.7s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binning 0.001 GB of training data: 0.111 s\n",
      "Binning 0.000 GB of validation data: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.019 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00219, val score: -0.00217, in 0.103s\n",
      "[2/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00184, val score: -0.00182, in 0.118s\n",
      "[3/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00164, val score: -0.00163, in 0.143s\n",
      "[4/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00153, val score: -0.00152, in 0.133s\n",
      "[5/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00147, val score: -0.00146, in 0.125s\n",
      "[6/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00143, val score: -0.00143, in 0.182s\n",
      "[7/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00142, val score: -0.00141, in 0.159s\n",
      "[8/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00141, val score: -0.00140, in 0.173s\n",
      "[9/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00140, val score: -0.00139, in 0.165s\n",
      "[10/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00140, val score: -0.00139, in 0.169s\n",
      "[11/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00139, in 0.187s\n",
      "[12/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00139, val score: -0.00139, in 0.182s\n",
      "[13/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.179s\n",
      "[14/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00139, val score: -0.00138, in 0.192s\n",
      "[15/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.195s\n",
      "[16/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.172s\n",
      "[17/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00139, val score: -0.00138, in 0.197s\n",
      "[18/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00139, val score: -0.00138, in 0.199s\n",
      "[19/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.219s\n",
      "[20/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.226s\n",
      "[21/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.222s\n",
      "[22/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.232s\n",
      "[23/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.248s\n",
      "[24/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00139, val score: -0.00138, in 0.240s\n",
      "[25/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00139, val score: -0.00138, in 0.243s\n",
      "[26/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00139, val score: -0.00138, in 0.255s\n",
      "[27/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.247s\n",
      "Fit 27 trees in 5.401 s, (405 total leaves)\n",
      "Time spent computing histograms: 0.275s\n",
      "Time spent finding best splits:  0.025s\n",
      "Time spent applying splits:      0.284s\n",
      "Time spent predicting:           0.006s\n",
      "Binning 0.001 GB of training data: 0.121 s\n",
      "Binning 0.000 GB of validation data: 0.018 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00219, val score: -0.00217, in 0.092s\n",
      "[2/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00184, val score: -0.00182, in 0.125s\n",
      "[3/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00164, val score: -0.00163, in 0.122s\n",
      "[4/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00153, val score: -0.00152, in 0.146s\n",
      "[5/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00147, val score: -0.00146, in 0.143s\n",
      "[6/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00143, val score: -0.00143, in 0.137s\n",
      "[7/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00142, val score: -0.00141, in 0.139s\n",
      "[8/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00141, val score: -0.00140, in 0.149s\n",
      "[9/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00140, val score: -0.00139, in 0.150s\n",
      "[10/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00140, val score: -0.00139, in 0.161s\n",
      "[11/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00139, in 0.143s\n",
      "[12/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00139, val score: -0.00139, in 0.172s\n",
      "[13/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.179s\n",
      "[14/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00139, val score: -0.00138, in 0.174s\n",
      "[15/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.185s\n",
      "[16/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.180s\n",
      "[17/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00139, val score: -0.00138, in 0.198s\n",
      "[18/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00139, val score: -0.00138, in 0.205s\n",
      "[19/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.218s\n",
      "[20/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.211s\n",
      "[21/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.203s\n",
      "[22/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.221s\n",
      "[23/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.244s\n",
      "[24/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00139, val score: -0.00138, in 0.222s\n",
      "[25/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00139, val score: -0.00138, in 0.230s\n",
      "[26/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00139, val score: -0.00138, in 0.261s\n",
      "[27/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00139, val score: -0.00138, in 0.242s\n",
      "Fit 27 trees in 5.237 s, (405 total leaves)\n",
      "Time spent computing histograms: 0.259s\n",
      "Time spent finding best splits:  0.032s\n",
      "Time spent applying splits:      0.253s\n",
      "Time spent predicting:           0.006s\n",
      "Train R2: 0.767, Train RMSPE: 0.001, Validation R2: 0.769, Validation RMSPE: 0.001\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "grid_search.best_params_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'min_samples_leaf': 30,\n",
       " 'max_leaf_nodes': 15,\n",
       " 'max_depth': None,\n",
       " 'learning_rate': 0.28117686979742307,\n",
       " 'l2_regularization': 0.0}"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preds = predict_avg(models, inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "model = HistGradientBoostingRegressor(random_state=11111,scoring='neg_root_mean_squared_error',validation_fraction=0.2,verbose=1,loss='least_squares')\r\n",
    "param_grid = {#\"max_depth\": [10, 20, 30, 40, 50, None],\r\n",
    "              \"max_leaf_nodes\" : [7,15,31,63],\r\n",
    "              \"learning_rate\": np.logspace(-1,1,10),\r\n",
    "              \"min_samples_leaf\": [20,30,40,50],\r\n",
    "              #\"l2_regularization\": [0.0,0.00001,0.0001,0.001,0.01]\r\n",
    "            }\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "models = []\r\n",
    "for train_idxs, val_idxs in ss.split(inputs):\r\n",
    "    X_train, train_targets = inputs.iloc[train_idxs], targets.iloc[train_idxs]\r\n",
    "    X_val, val_targets = inputs.iloc[val_idxs], targets.iloc[val_idxs]\r\n",
    "    X_train = np.ascontiguousarray(X_train).reshape(-1,2)\r\n",
    "    train_targets = np.ascontiguousarray(train_targets).reshape(-1,1)\r\n",
    "    X_val = np.ascontiguousarray(X_val).reshape(-1,2)\r\n",
    "    val_targets = np.ascontiguousarray(val_targets).reshape(-1,1)\r\n",
    "    grid_search = GridSearchCV(model, param_grid,  scoring='r2',verbose=1,return_train_score=True,n_jobs=-1).fit(np.X_train,train_targets)\r\n",
    "    model, train_R2, train_RMSPE, val_R2, val_RMSPE = train_and_evaluate(X_train, \r\n",
    "                                                        train_targets, \r\n",
    "                                                        X_val, \r\n",
    "                                                        val_targets, \r\n",
    "                                                        **grid_search.best_params_)\r\n",
    "    models.append(model)\r\n",
    "    print('Train R2: {}, Train RMSPE: {}, Validation R2: {}, Validation RMSPE: {}'.format(train_R2, train_RMSPE, val_R2, val_RMSPE))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  3.7min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binning 0.001 GB of training data: 0.134 s\n",
      "Binning 0.000 GB of validation data: 0.024 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00248, val score: -0.00248, in 0.150s\n",
      "[2/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00221, val score: -0.00221, in 0.126s\n",
      "[3/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00200, val score: -0.00200, in 0.166s\n",
      "[4/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00184, val score: -0.00183, in 0.146s\n",
      "[5/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00172, val score: -0.00171, in 0.182s\n",
      "[6/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00162, val score: -0.00162, in 0.340s\n",
      "[7/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00156, val score: -0.00155, in 0.204s\n",
      "[8/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00150, val score: -0.00149, in 0.182s\n",
      "[9/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00147, val score: -0.00146, in 0.154s\n",
      "[10/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00144, val score: -0.00143, in 0.192s\n",
      "[11/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00142, val score: -0.00141, in 0.160s\n",
      "[12/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00140, in 0.204s\n",
      "[13/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00139, val score: -0.00138, in 0.181s\n",
      "[14/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00139, val score: -0.00138, in 0.203s\n",
      "[15/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00138, val score: -0.00137, in 0.228s\n",
      "[16/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00137, in 0.232s\n",
      "[17/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00137, in 0.200s\n",
      "[18/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.249s\n",
      "[19/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.243s\n",
      "[20/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00136, in 0.256s\n",
      "[21/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.266s\n",
      "[22/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00136, in 0.262s\n",
      "[23/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.268s\n",
      "[24/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.296s\n",
      "[25/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.292s\n",
      "[26/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.281s\n",
      "[27/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00136, val score: -0.00136, in 0.323s\n",
      "[28/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.325s\n",
      "[29/100] 1 tree, 7 leaves, max depth = 6, train score: -0.00136, val score: -0.00136, in 0.314s\n",
      "[30/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.323s\n",
      "[31/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.329s\n",
      "[32/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00136, val score: -0.00136, in 0.319s\n",
      "[33/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.353s\n",
      "[34/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00136, val score: -0.00136, in 0.345s\n",
      "[35/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.361s\n",
      "[36/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00136, val score: -0.00136, in 0.360s\n",
      "[37/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.370s\n",
      "[38/100] 1 tree, 7 leaves, max depth = 6, train score: -0.00136, val score: -0.00136, in 0.378s\n",
      "[39/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00136, val score: -0.00136, in 0.399s\n",
      "[40/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.391s\n",
      "[41/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00136, val score: -0.00136, in 0.426s\n",
      "Fit 41 trees in 12.178 s, (287 total leaves)\n",
      "Time spent computing histograms: 0.526s\n",
      "Time spent finding best splits:  0.097s\n",
      "Time spent applying splits:      0.422s\n",
      "Time spent predicting:           0.009s\n",
      "Binning 0.001 GB of training data: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.117 s\n",
      "Binning 0.000 GB of validation data: 0.021 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00245, val score: -0.00244, in 0.092s\n",
      "[2/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00217, val score: -0.00217, in 0.136s\n",
      "[3/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00196, val score: -0.00196, in 0.140s\n",
      "[4/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00181, val score: -0.00180, in 0.153s\n",
      "[5/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00169, val score: -0.00168, in 0.159s\n",
      "[6/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00160, val score: -0.00160, in 0.161s\n",
      "[7/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00154, val score: -0.00153, in 0.149s\n",
      "[8/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00149, val score: -0.00149, in 0.191s\n",
      "[9/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00146, val score: -0.00145, in 0.178s\n",
      "[10/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00143, val score: -0.00143, in 0.181s\n",
      "[11/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00142, val score: -0.00141, in 0.160s\n",
      "[12/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00140, in 0.287s\n",
      "[13/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00139, in 0.206s\n",
      "[14/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00139, val score: -0.00138, in 0.186s\n",
      "[15/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00138, val score: -0.00137, in 0.231s\n",
      "[16/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00137, in 0.227s\n",
      "[17/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00137, in 0.233s\n",
      "[18/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00137, in 0.216s\n",
      "[19/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00136, in 0.230s\n",
      "[20/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00136, in 0.222s\n",
      "[21/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00137, val score: -0.00136, in 0.223s\n",
      "[22/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00137, val score: -0.00136, in 0.253s\n",
      "[23/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.306s\n",
      "[24/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.292s\n",
      "[25/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.318s\n",
      "[26/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00136, in 0.268s\n",
      "[27/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00137, val score: -0.00136, in 0.278s\n",
      "[28/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.241s\n",
      "[29/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00136, in 0.285s\n",
      "[30/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.273s\n",
      "[31/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00136, in 0.285s\n",
      "[32/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.299s\n",
      "[33/100] 1 tree, 7 leaves, max depth = 6, train score: -0.00137, val score: -0.00136, in 0.259s\n",
      "[34/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.323s\n",
      "[35/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.253s\n",
      "[36/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00136, in 0.300s\n",
      "[37/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00136, in 0.287s\n",
      "[38/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00136, in 0.340s\n",
      "[39/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00136, in 0.287s\n",
      "[40/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.361s\n",
      "[41/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.408s\n",
      "[42/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.387s\n",
      "[43/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00136, val score: -0.00136, in 0.346s\n",
      "[44/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.382s\n",
      "[45/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.326s\n",
      "[46/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00136, val score: -0.00136, in 0.377s\n",
      "[47/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.373s\n",
      "[48/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.359s\n",
      "[49/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00136, val score: -0.00136, in 0.399s\n",
      "[50/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00136, val score: -0.00136, in 0.440s\n",
      "[51/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00136, val score: -0.00136, in 0.424s\n",
      "[52/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00136, val score: -0.00136, in 0.442s\n",
      "Fit 52 trees in 14.752 s, (364 total leaves)\n",
      "Time spent computing histograms: 0.372s\n",
      "Time spent finding best splits:  0.083s\n",
      "Time spent applying splits:      0.371s\n",
      "Time spent predicting:           0.010s\n",
      "Train R2: 0.769, Train RMSPE: 0.001, Validation R2: 0.759, Validation RMSPE: 0.001\n",
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  4.4min\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binning 0.001 GB of training data: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  4.4min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: RuntimeWarning: invalid value encountered in subtract\n",
      "  array_stds = np.sqrt(np.average((array -\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.112 s\n",
      "Binning 0.000 GB of validation data: 0.028 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00243, val score: -0.00246, in 0.194s\n",
      "[2/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00215, val score: -0.00218, in 0.178s\n",
      "[3/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00193, val score: -0.00197, in 0.169s\n",
      "[4/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00177, val score: -0.00181, in 0.302s\n",
      "[5/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00165, val score: -0.00169, in 0.200s\n",
      "[6/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00157, val score: -0.00161, in 0.179s\n",
      "[7/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00150, val score: -0.00155, in 0.166s\n",
      "[8/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00146, val score: -0.00150, in 0.160s\n",
      "[9/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00143, val score: -0.00147, in 0.155s\n",
      "[10/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00141, val score: -0.00145, in 0.152s\n",
      "[11/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00139, val score: -0.00143, in 0.152s\n",
      "[12/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00138, val score: -0.00142, in 0.158s\n",
      "[13/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00137, val score: -0.00141, in 0.165s\n",
      "[14/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00136, val score: -0.00141, in 0.160s\n",
      "[15/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00136, val score: -0.00140, in 0.184s\n",
      "[16/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00136, val score: -0.00140, in 0.168s\n",
      "[17/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00135, val score: -0.00140, in 0.173s\n",
      "[18/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00135, val score: -0.00140, in 0.177s\n",
      "[19/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00135, val score: -0.00140, in 0.190s\n",
      "[20/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00135, val score: -0.00140, in 0.191s\n",
      "[21/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00135, val score: -0.00140, in 0.171s\n",
      "[22/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00135, val score: -0.00140, in 0.198s\n",
      "[23/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00135, val score: -0.00140, in 0.209s\n",
      "[24/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00135, val score: -0.00140, in 0.218s\n",
      "[25/100] 1 tree, 15 leaves, max depth = 10, train score: -0.00135, val score: -0.00140, in 0.189s\n",
      "[26/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00135, val score: -0.00140, in 0.222s\n",
      "[27/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00135, val score: -0.00140, in 0.231s\n",
      "[28/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00135, val score: -0.00139, in 0.249s\n",
      "[29/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00135, val score: -0.00139, in 0.255s\n",
      "[30/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00135, val score: -0.00139, in 0.306s\n",
      "[31/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00135, val score: -0.00139, in 0.305s\n",
      "[32/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00134, val score: -0.00139, in 0.269s\n",
      "[33/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00134, val score: -0.00139, in 0.261s\n",
      "[34/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00134, val score: -0.00139, in 0.362s\n",
      "[35/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00134, val score: -0.00139, in 0.261s\n",
      "[36/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00134, val score: -0.00139, in 0.281s\n",
      "[37/100] 1 tree, 15 leaves, max depth = 10, train score: -0.00134, val score: -0.00139, in 0.268s\n",
      "[38/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00134, val score: -0.00139, in 0.281s\n",
      "Fit 38 trees in 8.479 s, (570 total leaves)\n",
      "Time spent computing histograms: 0.515s\n",
      "Time spent finding best splits:  0.141s\n",
      "Time spent applying splits:      0.433s\n",
      "Time spent predicting:           0.025s\n",
      "Binning 0.001 GB of training data: 0.076 s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binning 0.000 GB of validation data: 0.017 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00243, val score: -0.00246, in 0.066s\n",
      "[2/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00215, val score: -0.00218, in 0.094s\n",
      "[3/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00193, val score: -0.00197, in 0.098s\n",
      "[4/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00177, val score: -0.00181, in 0.131s\n",
      "[5/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00165, val score: -0.00169, in 0.109s\n",
      "[6/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00157, val score: -0.00161, in 0.117s\n",
      "[7/100] 1 tree, 15 leaves, max depth = 4, train score: -0.00150, val score: -0.00155, in 0.139s\n",
      "[8/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00146, val score: -0.00150, in 0.132s\n",
      "[9/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00143, val score: -0.00147, in 0.132s\n",
      "[10/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00141, val score: -0.00145, in 0.140s\n",
      "[11/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00139, val score: -0.00143, in 0.150s\n",
      "[12/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00138, val score: -0.00142, in 0.134s\n",
      "[13/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00137, val score: -0.00141, in 0.147s\n",
      "[14/100] 1 tree, 15 leaves, max depth = 5, train score: -0.00136, val score: -0.00141, in 0.168s\n",
      "[15/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00136, val score: -0.00140, in 0.193s\n",
      "[16/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00136, val score: -0.00140, in 0.178s\n",
      "[17/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00135, val score: -0.00140, in 0.217s\n",
      "[18/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00135, val score: -0.00140, in 0.233s\n",
      "[19/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00135, val score: -0.00140, in 0.219s\n",
      "[20/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00135, val score: -0.00140, in 0.178s\n",
      "[21/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00135, val score: -0.00140, in 0.200s\n",
      "[22/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00135, val score: -0.00140, in 0.213s\n",
      "[23/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00135, val score: -0.00140, in 0.228s\n",
      "[24/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00135, val score: -0.00140, in 0.221s\n",
      "[25/100] 1 tree, 15 leaves, max depth = 10, train score: -0.00135, val score: -0.00140, in 0.234s\n",
      "[26/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00135, val score: -0.00140, in 0.220s\n",
      "[27/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00135, val score: -0.00140, in 0.246s\n",
      "[28/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00135, val score: -0.00139, in 0.252s\n",
      "[29/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00135, val score: -0.00139, in 0.322s\n",
      "[30/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00135, val score: -0.00139, in 0.329s\n",
      "[31/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00135, val score: -0.00139, in 0.304s\n",
      "[32/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00134, val score: -0.00139, in 0.331s\n",
      "[33/100] 1 tree, 15 leaves, max depth = 7, train score: -0.00134, val score: -0.00139, in 0.270s\n",
      "[34/100] 1 tree, 15 leaves, max depth = 9, train score: -0.00134, val score: -0.00139, in 0.253s\n",
      "[35/100] 1 tree, 15 leaves, max depth = 8, train score: -0.00134, val score: -0.00139, in 0.256s\n",
      "[36/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00134, val score: -0.00139, in 0.257s\n",
      "[37/100] 1 tree, 15 leaves, max depth = 10, train score: -0.00134, val score: -0.00139, in 0.337s\n",
      "[38/100] 1 tree, 15 leaves, max depth = 6, train score: -0.00134, val score: -0.00139, in 0.324s\n",
      "Fit 38 trees in 8.132 s, (570 total leaves)\n",
      "Time spent computing histograms: 0.349s\n",
      "Time spent finding best splits:  0.066s\n",
      "Time spent applying splits:      0.296s\n",
      "Time spent predicting:           0.017s\n",
      "Train R2: 0.771, Train RMSPE: 0.001, Validation R2: 0.757, Validation RMSPE: 0.001\n",
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   58.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  5.6min\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binning 0.001 GB of training data: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  5.6min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: RuntimeWarning: invalid value encountered in subtract\n",
      "  array_stds = np.sqrt(np.average((array -\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: RuntimeWarning: overflow encountered in square\n",
      "  array_stds = np.sqrt(np.average((array -\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.178 s\n",
      "Binning 0.000 GB of validation data: 0.091 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00266, val score: -0.00263, in 0.206s\n",
      "[2/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00247, val score: -0.00244, in 0.230s\n",
      "[3/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00231, val score: -0.00228, in 0.210s\n",
      "[4/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00218, val score: -0.00214, in 0.232s\n",
      "[5/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00206, val score: -0.00202, in 0.245s\n",
      "[6/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00196, val score: -0.00192, in 0.284s\n",
      "[7/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00187, val score: -0.00183, in 0.287s\n",
      "[8/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00180, val score: -0.00176, in 0.300s\n",
      "[9/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00173, val score: -0.00169, in 0.297s\n",
      "[10/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00168, val score: -0.00164, in 0.283s\n",
      "[11/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00164, val score: -0.00160, in 0.318s\n",
      "[12/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00160, val score: -0.00156, in 0.324s\n",
      "[13/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00157, val score: -0.00153, in 0.337s\n",
      "[14/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00154, val score: -0.00150, in 0.291s\n",
      "[15/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00152, val score: -0.00148, in 0.239s\n",
      "[16/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00150, val score: -0.00146, in 0.297s\n",
      "[17/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00149, val score: -0.00144, in 0.288s\n",
      "[18/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00147, val score: -0.00143, in 0.258s\n",
      "[19/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00146, val score: -0.00142, in 0.252s\n",
      "[20/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00145, val score: -0.00141, in 0.272s\n",
      "[21/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00145, val score: -0.00141, in 0.283s\n",
      "[22/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00144, val score: -0.00140, in 0.275s\n",
      "[23/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00144, val score: -0.00139, in 0.302s\n",
      "[24/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00143, val score: -0.00139, in 0.490s\n",
      "[25/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00143, val score: -0.00139, in 0.381s\n",
      "[26/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00142, val score: -0.00138, in 0.326s\n",
      "[27/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00142, val score: -0.00138, in 0.296s\n",
      "[28/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00142, val score: -0.00138, in 0.302s\n",
      "[29/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00142, val score: -0.00138, in 0.499s\n",
      "[30/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00137, in 0.346s\n",
      "[31/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.360s\n",
      "[32/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.366s\n",
      "[33/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.429s\n",
      "[34/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.375s\n",
      "[35/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.423s\n",
      "[36/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.373s\n",
      "[37/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.378s\n",
      "[38/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00137, in 0.385s\n",
      "[39/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.397s\n",
      "[40/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.401s\n",
      "[41/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.401s\n",
      "[42/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.385s\n",
      "[43/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.387s\n",
      "[44/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00137, in 0.521s\n",
      "[45/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.476s\n",
      "[46/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.368s\n",
      "[47/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00137, in 0.464s\n",
      "[48/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.431s\n",
      "[49/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00137, in 0.421s\n",
      "[50/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.425s\n",
      "[51/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00140, val score: -0.00137, in 0.452s\n",
      "[52/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00140, val score: -0.00137, in 0.441s\n",
      "[53/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.396s\n",
      "[54/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.413s\n",
      "[55/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00140, val score: -0.00137, in 0.576s\n",
      "[56/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.495s\n",
      "[57/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00140, val score: -0.00137, in 0.473s\n",
      "[58/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.430s\n",
      "[59/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.501s\n",
      "[60/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00140, val score: -0.00137, in 0.599s\n",
      "Fit 60 trees in 22.569 s, (420 total leaves)\n",
      "Time spent computing histograms: 0.739s\n",
      "Time spent finding best splits:  0.251s\n",
      "Time spent applying splits:      0.909s\n",
      "Time spent predicting:           0.046s\n",
      "Binning 0.001 GB of training data: 0.085 s\n",
      "Binning 0.000 GB of validation data: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.020 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00266, val score: -0.00263, in 0.066s\n",
      "[2/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00247, val score: -0.00244, in 0.117s\n",
      "[3/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00231, val score: -0.00228, in 0.126s\n",
      "[4/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00218, val score: -0.00214, in 0.163s\n",
      "[5/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00206, val score: -0.00202, in 0.166s\n",
      "[6/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00196, val score: -0.00192, in 0.146s\n",
      "[7/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00187, val score: -0.00183, in 0.134s\n",
      "[8/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00180, val score: -0.00176, in 0.147s\n",
      "[9/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00173, val score: -0.00169, in 0.150s\n",
      "[10/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00168, val score: -0.00164, in 0.163s\n",
      "[11/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00164, val score: -0.00160, in 0.172s\n",
      "[12/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00160, val score: -0.00156, in 0.158s\n",
      "[13/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00157, val score: -0.00153, in 0.196s\n",
      "[14/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00154, val score: -0.00150, in 0.240s\n",
      "[15/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00152, val score: -0.00148, in 0.242s\n",
      "[16/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00150, val score: -0.00146, in 0.234s\n",
      "[17/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00149, val score: -0.00144, in 0.254s\n",
      "[18/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00147, val score: -0.00143, in 0.245s\n",
      "[19/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00146, val score: -0.00142, in 0.213s\n",
      "[20/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00145, val score: -0.00141, in 0.244s\n",
      "[21/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00145, val score: -0.00141, in 0.232s\n",
      "[22/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00144, val score: -0.00140, in 0.238s\n",
      "[23/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00144, val score: -0.00139, in 0.239s\n",
      "[24/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00143, val score: -0.00139, in 0.249s\n",
      "[25/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00143, val score: -0.00139, in 0.254s\n",
      "[26/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00142, val score: -0.00138, in 0.249s\n",
      "[27/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00142, val score: -0.00138, in 0.235s\n",
      "[28/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00142, val score: -0.00138, in 0.255s\n",
      "[29/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00142, val score: -0.00138, in 0.257s\n",
      "[30/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00137, in 0.237s\n",
      "[31/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.272s\n",
      "[32/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.254s\n",
      "[33/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.279s\n",
      "[34/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.340s\n",
      "[35/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.291s\n",
      "[36/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.277s\n",
      "[37/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.274s\n",
      "[38/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00137, in 0.263s\n",
      "[39/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.282s\n",
      "[40/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.369s\n",
      "[41/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.371s\n",
      "[42/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00141, val score: -0.00137, in 0.387s\n",
      "[43/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.417s\n",
      "[44/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00137, in 0.389s\n",
      "[45/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.382s\n",
      "[46/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.376s\n",
      "[47/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00137, in 0.351s\n",
      "[48/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.414s\n",
      "[49/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00137, in 0.409s\n",
      "[50/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.397s\n",
      "[51/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00140, val score: -0.00137, in 0.453s\n",
      "[52/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00140, val score: -0.00137, in 0.443s\n",
      "[53/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.520s\n",
      "[54/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.445s\n",
      "[55/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00140, val score: -0.00137, in 0.441s\n",
      "[56/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.424s\n",
      "[57/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00140, val score: -0.00137, in 0.414s\n",
      "[58/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.414s\n",
      "[59/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00137, in 0.461s\n",
      "[60/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00140, val score: -0.00137, in 0.526s\n",
      "Fit 60 trees in 17.828 s, (420 total leaves)\n",
      "Time spent computing histograms: 0.414s\n",
      "Time spent finding best splits:  0.034s\n",
      "Time spent applying splits:      0.461s\n",
      "Time spent predicting:           0.012s\n",
      "Train R2: 0.767, Train RMSPE: 0.001, Validation R2: 0.765, Validation RMSPE: 0.001\n",
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  5.3min\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binning 0.001 GB of training data: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  5.3min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: RuntimeWarning: invalid value encountered in subtract\n",
      "  array_stds = np.sqrt(np.average((array -\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.133 s\n",
      "Binning 0.000 GB of validation data: 0.024 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00262, val score: -0.00264, in 0.184s\n",
      "[2/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00243, val score: -0.00245, in 0.179s\n",
      "[3/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00227, val score: -0.00229, in 0.210s\n",
      "[4/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00213, val score: -0.00215, in 0.191s\n",
      "[5/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00201, val score: -0.00203, in 0.203s\n",
      "[6/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00191, val score: -0.00193, in 0.240s\n",
      "[7/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00182, val score: -0.00184, in 0.236s\n",
      "[8/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00175, val score: -0.00177, in 0.246s\n",
      "[9/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00169, val score: -0.00170, in 0.236s\n",
      "[10/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00163, val score: -0.00165, in 0.264s\n",
      "[11/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00159, val score: -0.00160, in 0.282s\n",
      "[12/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00155, val score: -0.00157, in 0.282s\n",
      "[13/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00152, val score: -0.00154, in 0.210s\n",
      "[14/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00150, val score: -0.00151, in 0.214s\n",
      "[15/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00147, val score: -0.00149, in 0.218s\n",
      "[16/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00146, val score: -0.00147, in 0.200s\n",
      "[17/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00144, val score: -0.00146, in 0.242s\n",
      "[18/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00143, val score: -0.00145, in 0.226s\n",
      "[19/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00142, val score: -0.00144, in 0.202s\n",
      "[20/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00143, in 0.226s\n",
      "[21/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00142, in 0.222s\n",
      "[22/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00142, in 0.220s\n",
      "[23/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00140, val score: -0.00141, in 0.253s\n",
      "[24/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00139, val score: -0.00141, in 0.269s\n",
      "[25/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00139, val score: -0.00140, in 0.273s\n",
      "[26/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00139, val score: -0.00140, in 0.255s\n",
      "[27/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00140, in 0.301s\n",
      "[28/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00140, in 0.300s\n",
      "[29/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00140, in 0.282s\n",
      "[30/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00140, in 0.292s\n",
      "[31/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00139, in 0.343s\n",
      "[32/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00138, val score: -0.00139, in 0.322s\n",
      "[33/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00139, in 0.300s\n",
      "[34/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00139, in 0.306s\n",
      "[35/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00139, in 0.316s\n",
      "[36/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00139, in 0.300s\n",
      "[37/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00139, in 0.323s\n",
      "[38/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00139, in 0.333s\n",
      "[39/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00137, val score: -0.00139, in 0.328s\n",
      "[40/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.345s\n",
      "[41/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.340s\n",
      "[42/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.360s\n",
      "[43/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.359s\n",
      "[44/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.361s\n",
      "[45/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.397s\n",
      "[46/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.469s\n",
      "[47/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.397s\n",
      "[48/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00139, in 0.369s\n",
      "[49/100] 1 tree, 7 leaves, max depth = 6, train score: -0.00137, val score: -0.00139, in 0.404s\n",
      "[50/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.394s\n",
      "[51/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.414s\n",
      "[52/100] 1 tree, 7 leaves, max depth = 6, train score: -0.00137, val score: -0.00139, in 0.410s\n",
      "[53/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00137, val score: -0.00139, in 0.425s\n",
      "[54/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00139, in 0.407s\n",
      "Fit 54 trees in 16.426 s, (378 total leaves)\n",
      "Time spent computing histograms: 0.601s\n",
      "Time spent finding best splits:  0.124s\n",
      "Time spent applying splits:      0.583s\n",
      "Time spent predicting:           0.032s\n",
      "Binning 0.001 GB of training data: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.090 s\n",
      "Binning 0.000 GB of validation data: 0.015 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00262, val score: -0.00264, in 0.076s\n",
      "[2/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00243, val score: -0.00245, in 0.122s\n",
      "[3/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00227, val score: -0.00229, in 0.128s\n",
      "[4/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00213, val score: -0.00215, in 0.122s\n",
      "[5/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00201, val score: -0.00203, in 0.144s\n",
      "[6/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00191, val score: -0.00193, in 0.131s\n",
      "[7/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00182, val score: -0.00184, in 0.143s\n",
      "[8/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00175, val score: -0.00177, in 0.184s\n",
      "[9/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00169, val score: -0.00170, in 0.171s\n",
      "[10/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00163, val score: -0.00165, in 0.197s\n",
      "[11/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00159, val score: -0.00160, in 0.188s\n",
      "[12/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00155, val score: -0.00157, in 0.154s\n",
      "[13/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00152, val score: -0.00154, in 0.174s\n",
      "[14/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00150, val score: -0.00151, in 0.195s\n",
      "[15/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00147, val score: -0.00149, in 0.183s\n",
      "[16/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00146, val score: -0.00147, in 0.188s\n",
      "[17/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00144, val score: -0.00146, in 0.225s\n",
      "[18/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00143, val score: -0.00145, in 0.222s\n",
      "[19/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00142, val score: -0.00144, in 0.237s\n",
      "[20/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00143, in 0.230s\n",
      "[21/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00142, in 0.239s\n",
      "[22/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00142, in 0.245s\n",
      "[23/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00140, val score: -0.00141, in 0.247s\n",
      "[24/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00139, val score: -0.00141, in 0.257s\n",
      "[25/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00139, val score: -0.00140, in 0.279s\n",
      "[26/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00139, val score: -0.00140, in 0.263s\n",
      "[27/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00140, in 0.270s\n",
      "[28/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00140, in 0.283s\n",
      "[29/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00140, in 0.306s\n",
      "[30/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00140, in 0.357s\n",
      "[31/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00139, in 0.336s\n",
      "[32/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00138, val score: -0.00139, in 0.317s\n",
      "[33/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00139, in 0.302s\n",
      "[34/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00139, in 0.320s\n",
      "[35/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00139, in 0.320s\n",
      "[36/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00138, val score: -0.00139, in 0.313s\n",
      "[37/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00139, in 0.316s\n",
      "[38/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00138, val score: -0.00139, in 0.299s\n",
      "[39/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00137, val score: -0.00139, in 0.337s\n",
      "[40/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.334s\n",
      "[41/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.353s\n",
      "[42/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.364s\n",
      "[43/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.368s\n",
      "[44/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.345s\n",
      "[45/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.474s\n",
      "[46/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.394s\n",
      "[47/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.377s\n",
      "[48/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00139, in 0.382s\n",
      "[49/100] 1 tree, 7 leaves, max depth = 6, train score: -0.00137, val score: -0.00139, in 0.382s\n",
      "[50/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.407s\n",
      "[51/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00137, val score: -0.00139, in 0.402s\n",
      "[52/100] 1 tree, 7 leaves, max depth = 6, train score: -0.00137, val score: -0.00139, in 0.408s\n",
      "[53/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00137, val score: -0.00139, in 0.426s\n",
      "[54/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00137, val score: -0.00139, in 0.427s\n",
      "Fit 54 trees in 15.327 s, (378 total leaves)\n",
      "Time spent computing histograms: 0.319s\n",
      "Time spent finding best splits:  0.020s\n",
      "Time spent applying splits:      0.387s\n",
      "Time spent predicting:           0.019s\n",
      "Train R2: 0.766, Train RMSPE: 0.001, Validation R2: 0.768, Validation RMSPE: 0.001\n",
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  5.4min\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binning 0.001 GB of training data: 0.098 s\n",
      "Binning 0.000 GB of validation data: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  5.4min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: RuntimeWarning: invalid value encountered in subtract\n",
      "  array_stds = np.sqrt(np.average((array -\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.028 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00267, val score: -0.00264, in 0.194s\n",
      "[2/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00248, val score: -0.00245, in 0.192s\n",
      "[3/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00232, val score: -0.00229, in 0.180s\n",
      "[4/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00218, val score: -0.00215, in 0.196s\n",
      "[5/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00206, val score: -0.00203, in 0.189s\n",
      "[6/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00196, val score: -0.00193, in 0.207s\n",
      "[7/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00187, val score: -0.00184, in 0.179s\n",
      "[8/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00180, val score: -0.00177, in 0.206s\n",
      "[9/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00173, val score: -0.00170, in 0.210s\n",
      "[10/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00168, val score: -0.00165, in 0.192s\n",
      "[11/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00163, val score: -0.00160, in 0.195s\n",
      "[12/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00160, val score: -0.00157, in 0.226s\n",
      "[13/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00156, val score: -0.00154, in 0.141s\n",
      "[14/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00154, val score: -0.00151, in 0.153s\n",
      "[15/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00151, val score: -0.00149, in 0.202s\n",
      "[16/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00150, val score: -0.00147, in 0.195s\n",
      "[17/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00148, val score: -0.00145, in 0.228s\n",
      "[18/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00147, val score: -0.00144, in 0.188s\n",
      "[19/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00145, val score: -0.00143, in 0.191s\n",
      "[20/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00144, val score: -0.00142, in 0.155s\n",
      "[21/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00144, val score: -0.00141, in 0.160s\n",
      "[22/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00143, val score: -0.00141, in 0.170s\n",
      "[23/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00143, val score: -0.00140, in 0.167s\n",
      "[24/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00142, val score: -0.00140, in 0.159s\n",
      "[25/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00142, val score: -0.00139, in 0.208s\n",
      "[26/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00139, in 0.243s\n",
      "[27/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00139, in 0.253s\n",
      "[28/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00141, val score: -0.00139, in 0.260s\n",
      "[29/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00139, in 0.231s\n",
      "[30/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.200s\n",
      "[31/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.238s\n",
      "[32/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.197s\n",
      "[33/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.206s\n",
      "[34/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.226s\n",
      "[35/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.207s\n",
      "[36/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.196s\n",
      "[37/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.226s\n",
      "[38/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.321s\n",
      "[39/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.349s\n",
      "[40/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.362s\n",
      "[41/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.371s\n",
      "[42/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.379s\n",
      "[43/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.383s\n",
      "[44/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.384s\n",
      "[45/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.371s\n",
      "[46/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.379s\n",
      "[47/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.415s\n",
      "[48/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.411s\n",
      "[49/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.393s\n",
      "[50/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.372s\n",
      "[51/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.317s\n",
      "[52/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00139, val score: -0.00138, in 0.313s\n",
      "[53/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.305s\n",
      "[54/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.318s\n",
      "[55/100] 1 tree, 7 leaves, max depth = 6, train score: -0.00139, val score: -0.00138, in 0.264s\n",
      "[56/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.324s\n",
      "[57/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.271s\n",
      "[58/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.349s\n",
      "[59/100] 1 tree, 7 leaves, max depth = 6, train score: -0.00139, val score: -0.00138, in 0.313s\n",
      "[60/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.461s\n",
      "[61/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.412s\n",
      "Fit 61 trees in 16.328 s, (427 total leaves)\n",
      "Time spent computing histograms: 0.639s\n",
      "Time spent finding best splits:  0.161s\n",
      "Time spent applying splits:      0.521s\n",
      "Time spent predicting:           0.020s\n",
      "Binning 0.001 GB of training data: 0.068 s\n",
      "Binning 0.000 GB of validation data: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.023 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00267, val score: -0.00264, in 0.056s\n",
      "[2/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00248, val score: -0.00245, in 0.053s\n",
      "[3/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00232, val score: -0.00229, in 0.081s\n",
      "[4/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00218, val score: -0.00215, in 0.090s\n",
      "[5/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00206, val score: -0.00203, in 0.108s\n",
      "[6/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00196, val score: -0.00193, in 0.096s\n",
      "[7/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00187, val score: -0.00184, in 0.087s\n",
      "[8/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00180, val score: -0.00177, in 0.102s\n",
      "[9/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00173, val score: -0.00170, in 0.082s\n",
      "[10/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00168, val score: -0.00165, in 0.108s\n",
      "[11/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00163, val score: -0.00160, in 0.113s\n",
      "[12/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00160, val score: -0.00157, in 0.129s\n",
      "[13/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00156, val score: -0.00154, in 0.140s\n",
      "[14/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00154, val score: -0.00151, in 0.113s\n",
      "[15/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00151, val score: -0.00149, in 0.175s\n",
      "[16/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00150, val score: -0.00147, in 0.151s\n",
      "[17/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00148, val score: -0.00145, in 0.158s\n",
      "[18/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00147, val score: -0.00144, in 0.141s\n",
      "[19/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00145, val score: -0.00143, in 0.161s\n",
      "[20/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00144, val score: -0.00142, in 0.164s\n",
      "[21/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00144, val score: -0.00141, in 0.177s\n",
      "[22/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00143, val score: -0.00141, in 0.150s\n",
      "[23/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00143, val score: -0.00140, in 0.177s\n",
      "[24/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00142, val score: -0.00140, in 0.184s\n",
      "[25/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00142, val score: -0.00139, in 0.198s\n",
      "[26/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00139, in 0.149s\n",
      "[27/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00139, in 0.202s\n",
      "[28/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00141, val score: -0.00139, in 0.201s\n",
      "[29/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00141, val score: -0.00139, in 0.173s\n",
      "[30/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.240s\n",
      "[31/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.296s\n",
      "[32/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.303s\n",
      "[33/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.245s\n",
      "[34/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.233s\n",
      "[35/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.205s\n",
      "[36/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.221s\n",
      "[37/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.215s\n",
      "[38/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.203s\n",
      "[39/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.228s\n",
      "[40/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.245s\n",
      "[41/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.202s\n",
      "[42/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.265s\n",
      "[43/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.240s\n",
      "[44/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.352s\n",
      "[45/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.395s\n",
      "[46/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.331s\n",
      "[47/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.280s\n",
      "[48/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.266s\n",
      "[49/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.325s\n",
      "[50/100] 1 tree, 7 leaves, max depth = 3, train score: -0.00140, val score: -0.00138, in 0.370s\n",
      "[51/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00140, val score: -0.00138, in 0.295s\n",
      "[52/100] 1 tree, 7 leaves, max depth = 4, train score: -0.00139, val score: -0.00138, in 0.314s\n",
      "[53/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.297s\n",
      "[54/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.286s\n",
      "[55/100] 1 tree, 7 leaves, max depth = 6, train score: -0.00139, val score: -0.00138, in 0.293s\n",
      "[56/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.285s\n",
      "[57/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.312s\n",
      "[58/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.308s\n",
      "[59/100] 1 tree, 7 leaves, max depth = 6, train score: -0.00139, val score: -0.00138, in 0.305s\n",
      "[60/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.322s\n",
      "[61/100] 1 tree, 7 leaves, max depth = 5, train score: -0.00139, val score: -0.00138, in 0.310s\n",
      "Fit 61 trees in 13.234 s, (427 total leaves)\n",
      "Time spent computing histograms: 0.408s\n",
      "Time spent finding best splits:  0.064s\n",
      "Time spent applying splits:      0.305s\n",
      "Time spent predicting:           0.013s\n",
      "Train R2: 0.766, Train RMSPE: 0.001, Validation R2: 0.77, Validation RMSPE: 0.001\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Discussion on cross validation designs\r\n",
    "\r\n",
    "https://www.kaggle.com/vishnurapps/undersanding-kfold-stratifiedkfold-and-groupkfold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
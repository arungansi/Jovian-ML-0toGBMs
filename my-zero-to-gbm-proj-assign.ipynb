{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### my-zero-to-gbm-proj-assign"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optiver Realized Volatility Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This dataset contains stock market data relevant to the practical execution of trades in the financial markets. In particular, it includes order book snapshots and executed trades. With one second resolution, it provides a uniquely fine grained look at the micro-structure of modern financial markets.\n",
    "\n",
    "This is a code competition where only the first few rows of the test set are available for download. The rows that are visible are intended to illustrate the hidden test set format and folder structure. The remainder will only be available to your notebook when it is submitted. The hidden test set contains data that can be used to construct features to predict roughly 150,000 target values. Loading the entire dataset will take slightly more than 3 GB of memory, by our estimation.\n",
    "\n",
    "This is also a forecasting competition, where the final private leaderboard will be determined using data gathered after the training period closes, which means that the public and private leaderboards will have zero overlap. During the active training stage of the competition a large fraction of the test data will be filler, intended only to ensure the hidden dataset has approximately the same size as the actual test data. The filler data will be removed entirely during the forecasting phase of the competition and replaced with real"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install jovian --upgrade --quiet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import jovian"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Description\n",
    "\n",
    "book_[train/test].parquet A parquet file partitioned by stock_id. Provides order book data on the most competitive buy and sell orders entered into the market. The top two levels of the book are shared. The first level of the book will be more competitive in price terms, it will then receive execution priority over the second level.\n",
    "\n",
    "stock_id - ID code for the stock. Not all stock IDs exist in every time bucket. Parquet coerces this column to the categorical data type when loaded; you may wish to convert it to int8.\n",
    "time_id - ID code for the time bucket. Time IDs are not necessarily sequential but are consistent across all stocks.\n",
    "seconds_in_bucket - Number of seconds from the start of the bucket, always starting from 0.\n",
    "bid_price[1/2] - Normalized prices of the most/second most competitive buy level.\n",
    "ask_price[1/2] - Normalized prices of the most/second most competitive sell level.\n",
    "bid_size[1/2] - The number of shares on the most/second most competitive buy level.\n",
    "ask_size[1/2] - The number of shares on the most/second most competitive sell level.\n",
    "trade_[train/test].parquet A parquet file partitioned by stock_id. Contains data on trades that actually executed. Usually, in the market, there are more passive buy/sell intention updates (book updates) than actual trades, therefore one may expect this file to be more sparse than the order book.\n",
    "\n",
    "stock_id - Same as above.\n",
    "time_id - Same as above.\n",
    "seconds_in_bucket - Same as above. Note that since trade and book data are taken from the same time window and trade data is more sparse in general, this field is not necessarily starting from 0.\n",
    "price - The average price of executed transactions happening in one second. Prices have been normalized and the average has been weighted by the number of shares traded in each transaction.\n",
    "size - The sum number of shares traded.\n",
    "order_count - The number of unique trade orders taking place.\n",
    "train.csv The ground truth values for the training set.\n",
    "\n",
    "stock_id - Same as above, but since this is a csv the column will load as an integer instead of categorical.\n",
    "time_id - Same as above.\n",
    "target - The realized volatility computed over the 10 minute window following the feature data under the same stock/time_id. There is no overlap between feature and target data. You can find more info in our tutorial notebook.\n",
    "test.csv Provides the mapping between the other data files and the submission file. As with other test files, most of the data is only available to your notebook upon submission with just the first few rows available for download.\n",
    "\n",
    "stock_id - Same as above.\n",
    "time_id - Same as above.\n",
    "row_id - Unique identifier for the submission row. There is one row for each existing time ID/stock ID pair. Each time window is not necessarily containing every individual stock.\n",
    "sample_submission.csv - A sample submission file in the correct format.\n",
    "\n",
    "row_id - Same as in test.csv.\n",
    "target - Same definition as in train.csv. The benchmark is using the median target value from train.csv."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Execute this to save new versions of the notebook\r\n",
    "#jovian.commit(project=\"my-zero-to-gbm-proj-assign\")\r\n",
    "jovian.commit(filename=\"my-zero-to-gbm-proj-assign.ipynb\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[jovian] Updating notebook \"arun-gansi/my-zero-to-gbm-proj-assign-da8f6\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/arun-gansi/my-zero-to-gbm-proj-assign-da8f6\u001b[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://jovian.ai/arun-gansi/my-zero-to-gbm-proj-assign-da8f6'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import plotly.express as px\r\n",
    "train = pd.read_csv('../../../data/optiver-realized-volatility-prediction/train.csv')\r\n",
    "train.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   stock_id  time_id    target\n",
       "0         0        5  0.004136\n",
       "1         0       11  0.001445\n",
       "2         0       16  0.002168\n",
       "3         0       31  0.002195\n",
       "4         0       62  0.001747"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Points to be addressed before ML modeling\n",
    "\n",
    "1. stock_id - ID code for the stock. <font color='red'> Not all stock IDs exist in every time bucket </font>. Parquet coerces this column to the categorical data type when loaded;  <font color='red'> you may wish to convert it to int8 </font>\n",
    "2. We have missing “seconds_in_bucket” field?.A: That means there is no related market activities during the last second. For book data you can also assume the top-2 level book shape stays the same as the last available book update within the gap seconds, or, in another word, <font color='red'> you can forward fill the missing data point for all field in book data.</font>\n",
    "3. I'm trying to make trade data fixed sized. Since missing seconds_in_bucket implies no trade happening within that one-second window, is it technically correct to resample trade data to 600 seconds and fill it with zeros?. Hi, it is correct to assume 0 for order count and size. Some assumptions are required for the price, though. A price of 0 might cause issues.\n",
    "4. the trade data at T seconds contains a 1-second aggregation of executed orders between [T, T+1 second]\n",
    "5. One time_id represents a unique 20-minutes trading window which is consistent across all stocks As an example, let’s say time_id = 1 is representing a window between 1900-01-01 12:00:00 and 1900-01-01 12:20:00, then the book data of all stocks for that time_id are is taken from the same window. The data in the first 10 minutes window is shared with all of you, while the order book data of the second 10-minutes is used to build the target for you to predict. The dataset is rolling in such a way that feature and target data will always have zero overlap. Note that time_id is randomly shuffled, so it will not contain any information other than serving as a bridge between different dataset.\n",
    "6. We can demonstrate the data structure in below way:\n",
    "<img src = https://www.optiver.com/wp-content/uploads/2021/05/DataBucketing.png>\n",
    "7. In our competition, we shared the last snapshot of order book for each second. Imagine you have a time_id starting from 1900-01-01 12:00:00, the book update data on seconds_in_bucket = 1 represents the last snapshot of order book update between 12:00:00 and 12:00:01. Similarly to order book data in terms of granularity, but the trade data represents the aggregation of all individual orders happened within one second.\n",
    "8. So per stock, under the same time_id, the trade data on seconds_in_bucket = 1 represents the aggregation of all individual executed orders between 12:00:00 and 12:00:01. The size is the sum of the size in each individual order, while the price is aggregated as a volume weighted average price of all trades. A straightforward WAP formula can be found on Investopedia.\n",
    "9. Q: Why we have missing “seconds_in_bucket” field?\n",
    "A: That means there is no related market activities during the last second.\n",
    "\n",
    "For book data you can also assume the top-2 level book shape stays the same as the last available book update within the gap seconds, or, in another word, you can forward fill the missing data point for all field in book data. For trade data, it implies no trade happening within that one-second window. One thing to note that trade data tends to be more sparse than book data in many cases.\n",
    "\n",
    "10. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Taking the first row of data, it implies that the realized vol of the target bucket for time_id 5, stock_id 0 is 0.004136. How does the book and trade data in feature bucket look like for us to build signals?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "book_example = pd.read_parquet('../../../data/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0')\r\n",
    "trade_example =  pd.read_parquet('../../../data/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=0')\r\n",
    "stock_id = '0'\r\n",
    "book_example = book_example[book_example['time_id']==5]\r\n",
    "book_example.loc[:,'stock_id'] = stock_id\r\n",
    "trade_example = trade_example[trade_example['time_id']==5]\r\n",
    "trade_example.loc[:,'stock_id'] = stock_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Book data snapshot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "book_example['time_id'].unique()\r\n",
    "book_example.head(3)\r\n",
    "# book_example.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Trade date snapshot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trade_example.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Realized volatility calculation\n",
    "\n",
    "our target is to predict short-term realized volatility. Although the order book and trade data for the target cannot be shared, we can still present the realized volatility calculation using the feature data we provided.\n",
    "\n",
    "As realized volatility is a statistical measure of price changes on a given stock, to calculate the price change we first need to have a stock valuation at the fixed interval (1 second). We will use weighted averaged price, or WAP, of the order book data we provided."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "book_example['bid_price'] = (book_example['bid_price1'] * book_example['bid_size1'] + book_example['bid_price2'] * book_example['bid_size2'])/(book_example['bid_size1'] + book_example['bid_size2'])\r\n",
    "book_example['bid_size'] = (book_example['bid_size1'] + book_example['bid_size2'])\r\n",
    "\r\n",
    "book_example['ask_price'] = (book_example['ask_price1'] * book_example['ask_size1'] + book_example['ask_price2'] * book_example['ask_size2'])/(book_example['ask_size1'] + book_example['ask_size2'])\r\n",
    "book_example['ask_size'] = (book_example['ask_size1'] + book_example['ask_size2'])\r\n",
    "\r\n",
    "book_example['wap'] = (book_example['bid_price'] * book_example['ask_size'] + book_example['ask_price'] * book_example['bid_size']) / (book_example['bid_size'] +  book_example['ask_size'])\r\n",
    "\r\n",
    "book_example['wap1'] = (book_example['bid_price1'] * book_example['ask_size1'] + book_example['ask_price1'] * book_example['bid_size1']) / (book_example['bid_size1'] +  book_example['ask_size1'])\r\n",
    "\r\n",
    "book_example['wap2'] = (book_example['bid_price2'] * book_example['ask_size2'] + book_example['ask_price2'] * book_example['bid_size2']) / (book_example['bid_size2'] +  book_example['ask_size2'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The WAP of the stock is plotted below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.line(book_example, x=\"seconds_in_bucket\", y=\"wap\", title='WAP of stock_id_0, time_id_5')\r\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Log returns\n",
    "\n",
    "How can we compare the price of a stock between yesterday and today?\n",
    "\n",
    "The easiest method would be to just take the difference. This is definitely the most intuitive way, however price differences are not always comparable across stocks. For example, let's assume that we have invested \\$ 1000 in both stock A and stock B and that stock A moves from  \\$ 100 to  \\$ 102 and stock B moves from  \\$ 10 to  \\$ 11. We had a total of 10 shares of A ( \\$1000 / \\$100=10 ) which led to a profit of  10⋅(\\$102−\\$100)=\\$20  and a total of 100 shares of B that yielded \\$100. So the price increase was larger for stock A, although the move was proportionally much larger for stock B.\n",
    "\n",
    "We can solve the above problem by dividing the move by the starting price of the stock, effectively computing the percentage change in price, also known as the stock return. In our example, the return for stock A was  \\$102−\\$100/\\$100=2% , while for stock B it was  \\$11−\\$10/\\$10=10% . The stock return coincides with the percentage change in our invested capital.\n",
    "\n",
    "Returns are widely used in finance, however log returns are preferred whenever some mathematical modelling is required. Calling  St  the price of the stock  S  at time  t , we can define the log return between  t1  and  t2  as:\n",
    "\n",
    " - r(t1,t2)=log(St2/St1)\n",
    " \n",
    "Usually, we look at log returns over fixed time intervals, so with 10-minute log return we mean  rt=rt−10min,t .\n",
    "\n",
    "Log returns present several advantages, for example:\n",
    "\n",
    "- they are additive across time  r(t1,t2) + r(t2,t3) = (rt1,t3) \n",
    "- regular returns cannot go below -100%, while log returns are not bounded\n",
    "\n",
    "Next we will compute the log return\n",
    "\n",
    "To compute the Log Return, we will simply take the logarithm of the ratio between two consecutive WAP. The first row will have an empty return as the previous book update is unknown, therefore the empty return data point will be dropped."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def log_return(list_stock_prices):\r\n",
    "    return np.log(list_stock_prices).diff()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "book_example.loc[:,'log_return'] = log_return(book_example['wap'])\r\n",
    "\r\n",
    "#~ means NOT \r\n",
    "# also the null condition is applied because when we .diff() method the first row will have empty value as we cannot have diff of return with respect to the previous time id as the first row is the very first time id\r\n",
    "book_example = book_example[~book_example['log_return'].isnull()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "book_example['log_return'].isnull().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's plot the tick-to-tick return of this instrument over this time bucket"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.line(book_example, x=\"seconds_in_bucket\", y=\"log_return\", title='Log return of stock_id_0, time_id_5')\r\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Realized volatility\n",
    "\n",
    "When we trade options, a valuable input to our models is the standard deviation of the stock log returns. The standard deviation will be different for log returns computed over longer or shorter intervals, for this reason it is usually normalized to a 1-year period and the annualized standard deviation is called volatility.\n",
    "\n",
    "In this competition, you will be given 10 minutes of book data and we ask you to predict what the volatility will be in the following 10 minutes. Volatility will be measured as follows:\n",
    "\n",
    "We will compute the log returns over all consecutive book updates and we define the realized volatility,  σ , as the squared root of the sum of squared log returns.\n",
    "\n",
    "\\sigma = \\sqrt{\\sum_{t}r_{t-1, t}^2}\n",
    " \n",
    "Where we use WAP as price of the stock to compute log returns.\n",
    "\n",
    "We want to keep definitions as simple and clear as possible, so that Kagglers without financial knowledge will not be penalized. So we are not annualizing the volatility and we are assuming that log returns have 0 mean.\n",
    "\n",
    "\n",
    "The realized vol of stock 0 in this feature bucket, will be:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def realized_volatility(series_log_return):\r\n",
    "    return np.sqrt(np.sum(series_log_return**2))\r\n",
    "realized_vol = realized_volatility(book_example['log_return'])\r\n",
    "print(f'Realized volatility for stock_id 0 on time_id 5 is {realized_vol}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive prediction: using past realized volatility as target\n",
    "\n",
    "A commonly known fact about volatility is that it tends to be autocorrelated. We can use this property to implement a naive model that just \"predicts\" realized volatility by using whatever the realized volatility was in the initial 10 minutes.\n",
    "\n",
    "Let's calculate the past realized volatility across the training set to see how predictive a single naive signal can be."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import glob\r\n",
    "list_order_book_file_train = glob.glob('../../../data/optiver-realized-volatility-prediction/book_train.parquet/*')\r\n",
    "#list_order_book_file_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the data is partitioned by stock_id (each stock id is one folder), we try to calculcate realized volatility stock by stock and combine them into one target file. Note that the stock id as the partition column is not present if we load the single file so we will remedy that manually. We will reuse the log return and realized volatility functions defined in the previous session."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_wap(df_stock_book):\r\n",
    "       df_stock_book['bid_price'] = (df_stock_book['bid_price1'] * df_stock_book['bid_size1'] + df_stock_book['bid_price2'] * df_stock_book['bid_size2'])/(df_stock_book['bid_size1'] + df_stock_book['bid_size2'])\r\n",
    "       df_stock_book['bid_size'] = (df_stock_book['bid_size1'] + df_stock_book['bid_size2'])\r\n",
    "\r\n",
    "       df_stock_book['ask_price'] = (df_stock_book['ask_price1'] * df_stock_book['ask_size1'] + df_stock_book['ask_price2'] * df_stock_book['ask_size2'])/(df_stock_book['ask_size1'] + df_stock_book['ask_size2'])\r\n",
    "       df_stock_book['ask_size'] = (df_stock_book['ask_size1'] + df_stock_book['ask_size2'])\r\n",
    "\r\n",
    "       df_stock_book['wap'] = (df_stock_book['bid_price'] * df_stock_book['ask_size'] + df_stock_book['ask_price'] * df_stock_book['bid_size']) / (df_stock_book['bid_size'] +  df_stock_book['ask_size'])\r\n",
    "       return df_stock_book['wap']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def realized_volatility_per_time_id(file_path, prediction_column_name):\r\n",
    "    df_book_data = pd.read_parquet(file_path)\r\n",
    "#    df_book_data['wap'] = (df_book_data['bid_price1'] * df_book_data['ask_size1']+df_book_data['ask_price1'] * df_book_data['bid_size1'])  / (\r\n",
    "#                                     df_book_data['bid_size1']+ df_book_data[\r\n",
    "#                                  'ask_size1'])\r\n",
    "    df_book_data['wap'] = compute_wap(df_book_data)\r\n",
    "    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\r\n",
    "    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\r\n",
    "    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\r\n",
    "    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\r\n",
    "    stock_id = file_path.split('=')[1]\r\n",
    "    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\r\n",
    "    return df_realized_vol_per_stock[['row_id',prediction_column_name]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_order_book = pd.DataFrame()\r\n",
    "for file in list_order_book_file_train:\r\n",
    "     df_stock_book = pd.read_parquet(file)\r\n",
    "     df_stock_book['stock_id'] = file.split('=')[1]\r\n",
    "     df_order_book = pd.concat([df_order_book,df_stock_book])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import random\r\n",
    "# # Get current state and store\r\n",
    "# state = random.getstate()\r\n",
    "# # set current state\r\n",
    "# random.setstate(state)\r\n",
    "random.seed(42)\r\n",
    "list_order_book_file_train_50 = random.sample(list_order_book_file_train, 50)\r\n",
    "list_order_book_file_train_50_old = list_order_book_file_train_50\r\n",
    "random.seed(42)\r\n",
    "list_order_book_file_train_50 = random.sample(list_order_book_file_train, 50)\r\n",
    "assert(list_order_book_file_train_50 == list_order_book_file_train_50_old)\r\n",
    "#list(zip(list_order_book_file_train_50,list_order_book_file_train_50_old))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df_order_book_50 = pd.DataFrame()\r\n",
    "\r\n",
    "for file in list_order_book_file_train_50:\r\n",
    "     df_stock_book = pd.read_parquet(file)\r\n",
    "     df_stock_book['stock_id'] = file.split('=')[1]\r\n",
    "     df_order_book_50 = pd.concat([df_order_book_50,df_stock_book])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "len(list_order_book_file_train_50)\r\n",
    "\r\n",
    "df_order_book_50.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   time_id  seconds_in_bucket  bid_price1  ask_price1  bid_price2  ask_price2  \\\n",
       "0        5                  0    1.000354    1.000472    1.000236    1.000591   \n",
       "1        5                  1    1.000354    1.000591    1.000236    1.000827   \n",
       "2        5                  2    1.000354    1.000591    1.000236    1.000827   \n",
       "3        5                  3    1.000236    1.000591    1.000118    1.000827   \n",
       "4        5                  4    1.000354    1.000591    1.000236    1.000827   \n",
       "\n",
       "   bid_size1  ask_size1  bid_size2  ask_size2 stock_id  \n",
       "0        101         40        201        321       68  \n",
       "1        301        100        201        100       68  \n",
       "2        201        100        301        100       68  \n",
       "3        301        200        201        100       68  \n",
       "4          1        300        483        100       68  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>stock_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000354</td>\n",
       "      <td>1.000472</td>\n",
       "      <td>1.000236</td>\n",
       "      <td>1.000591</td>\n",
       "      <td>101</td>\n",
       "      <td>40</td>\n",
       "      <td>201</td>\n",
       "      <td>321</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000354</td>\n",
       "      <td>1.000591</td>\n",
       "      <td>1.000236</td>\n",
       "      <td>1.000827</td>\n",
       "      <td>301</td>\n",
       "      <td>100</td>\n",
       "      <td>201</td>\n",
       "      <td>100</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000354</td>\n",
       "      <td>1.000591</td>\n",
       "      <td>1.000236</td>\n",
       "      <td>1.000827</td>\n",
       "      <td>201</td>\n",
       "      <td>100</td>\n",
       "      <td>301</td>\n",
       "      <td>100</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000236</td>\n",
       "      <td>1.000591</td>\n",
       "      <td>1.000118</td>\n",
       "      <td>1.000827</td>\n",
       "      <td>301</td>\n",
       "      <td>200</td>\n",
       "      <td>201</td>\n",
       "      <td>100</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000354</td>\n",
       "      <td>1.000591</td>\n",
       "      <td>1.000236</td>\n",
       "      <td>1.000827</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>483</td>\n",
       "      <td>100</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#There are 112 unique stcoks are there in th order book\r\n",
    "len(df_order_book_50.stock_id.unique())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "grouped = df_order_book_50.groupby('time_id')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "##The no of stocks being traded at a given time id is not the same across the time ids. This is because not all stocks are actively traded in all time ids\r\n",
    "# There are 28 TimeIds where some stocks are not traded at all. This is out of total 3830 TimeIds\r\n",
    "a_lt_50 = grouped.stock_id.nunique() < 50\r\n",
    "\r\n",
    "stock_short_timeid = grouped.stock_id.unique()[list(a_lt_50.values)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "stock_short_timeid.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(27,)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "list(df_order_book_50.index)[-1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1834155"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#grouped.filter(a_lt_112)\r\n",
    "#len(df_order_book_50.loc[df_order_book_50.seconds_in_bucket == 0])\r\n",
    "print(df_order_book_50.time_id.nunique(),df_order_book_50.stock_id.nunique(),df_order_book_50.seconds_in_bucket.nunique())\r\n",
    "#There are a total of 3830 time series, 112 stocks and 600 seconds bucket \r\n",
    "# In this 3830 Times Ids there are a maximum possible order booking of 3830*600 = 2,298,000. But the actual is  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3830 50 600\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# with open('book_index.csv','w+') as f:\r\n",
    "#     for items in list(df_order_book_50.index):\r\n",
    "#         f.write('%s\\n' %items)\r\n",
    "#new_index\r\n",
    "df_order_book_50.reset_index(inplace=True)\r\n",
    "#df_order_book_50.head(-1)\r\n",
    "#df_order_book_50[df_order_book_50.index != df_order_book_50['index']]\r\n",
    "#(72311913, 11)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "df_order_book_50_idxed = df_order_book_50.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "new_index = pd.Index(np.arange(0,600), name=\"seconds_in_bucket\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df_order_book_50_idxed.astype({'stock_id':np.int8})"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            index  time_id  seconds_in_bucket  bid_price1  ask_price1  \\\n",
       "0               0        5                  0    1.000354    1.000472   \n",
       "1               1        5                  1    1.000354    1.000591   \n",
       "2               2        5                  2    1.000354    1.000591   \n",
       "3               3        5                  3    1.000236    1.000591   \n",
       "4               4        5                  4    1.000354    1.000591   \n",
       "...           ...      ...                ...         ...         ...   \n",
       "72311908  1834151    32767                593    1.002166    1.003610   \n",
       "72311909  1834152    32767                594    1.002166    1.003610   \n",
       "72311910  1834153    32767                595    1.002166    1.003610   \n",
       "72311911  1834154    32767                596    1.002166    1.003610   \n",
       "72311912  1834155    32767                597    1.002166    1.003610   \n",
       "\n",
       "          bid_price2  ask_price2  bid_size1  ask_size1  bid_size2  ask_size2  \\\n",
       "0           1.000236    1.000591        101         40        201        321   \n",
       "1           1.000236    1.000827        301        100        201        100   \n",
       "2           1.000236    1.000827        201        100        301        100   \n",
       "3           1.000118    1.000827        301        200        201        100   \n",
       "4           1.000236    1.000827          1        300        483        100   \n",
       "...              ...         ...        ...        ...        ...        ...   \n",
       "72311908    1.000722    1.005054      52200      66235      82550      64431   \n",
       "72311909    1.000722    1.005054      53700      66235      81050      64431   \n",
       "72311910    1.000722    1.005054      52200      66235      82550      64431   \n",
       "72311911    1.000722    1.005054      55700      66235      81050      64431   \n",
       "72311912    1.000722    1.005054      54200      66235      82550      64431   \n",
       "\n",
       "          stock_id  \n",
       "0               68  \n",
       "1               68  \n",
       "2               68  \n",
       "3               68  \n",
       "4               68  \n",
       "...            ...  \n",
       "72311908        31  \n",
       "72311909        31  \n",
       "72311910        31  \n",
       "72311911        31  \n",
       "72311912        31  \n",
       "\n",
       "[72311913 rows x 12 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>time_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>stock_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000354</td>\n",
       "      <td>1.000472</td>\n",
       "      <td>1.000236</td>\n",
       "      <td>1.000591</td>\n",
       "      <td>101</td>\n",
       "      <td>40</td>\n",
       "      <td>201</td>\n",
       "      <td>321</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000354</td>\n",
       "      <td>1.000591</td>\n",
       "      <td>1.000236</td>\n",
       "      <td>1.000827</td>\n",
       "      <td>301</td>\n",
       "      <td>100</td>\n",
       "      <td>201</td>\n",
       "      <td>100</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000354</td>\n",
       "      <td>1.000591</td>\n",
       "      <td>1.000236</td>\n",
       "      <td>1.000827</td>\n",
       "      <td>201</td>\n",
       "      <td>100</td>\n",
       "      <td>301</td>\n",
       "      <td>100</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000236</td>\n",
       "      <td>1.000591</td>\n",
       "      <td>1.000118</td>\n",
       "      <td>1.000827</td>\n",
       "      <td>301</td>\n",
       "      <td>200</td>\n",
       "      <td>201</td>\n",
       "      <td>100</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000354</td>\n",
       "      <td>1.000591</td>\n",
       "      <td>1.000236</td>\n",
       "      <td>1.000827</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>483</td>\n",
       "      <td>100</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72311908</th>\n",
       "      <td>1834151</td>\n",
       "      <td>32767</td>\n",
       "      <td>593</td>\n",
       "      <td>1.002166</td>\n",
       "      <td>1.003610</td>\n",
       "      <td>1.000722</td>\n",
       "      <td>1.005054</td>\n",
       "      <td>52200</td>\n",
       "      <td>66235</td>\n",
       "      <td>82550</td>\n",
       "      <td>64431</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72311909</th>\n",
       "      <td>1834152</td>\n",
       "      <td>32767</td>\n",
       "      <td>594</td>\n",
       "      <td>1.002166</td>\n",
       "      <td>1.003610</td>\n",
       "      <td>1.000722</td>\n",
       "      <td>1.005054</td>\n",
       "      <td>53700</td>\n",
       "      <td>66235</td>\n",
       "      <td>81050</td>\n",
       "      <td>64431</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72311910</th>\n",
       "      <td>1834153</td>\n",
       "      <td>32767</td>\n",
       "      <td>595</td>\n",
       "      <td>1.002166</td>\n",
       "      <td>1.003610</td>\n",
       "      <td>1.000722</td>\n",
       "      <td>1.005054</td>\n",
       "      <td>52200</td>\n",
       "      <td>66235</td>\n",
       "      <td>82550</td>\n",
       "      <td>64431</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72311911</th>\n",
       "      <td>1834154</td>\n",
       "      <td>32767</td>\n",
       "      <td>596</td>\n",
       "      <td>1.002166</td>\n",
       "      <td>1.003610</td>\n",
       "      <td>1.000722</td>\n",
       "      <td>1.005054</td>\n",
       "      <td>55700</td>\n",
       "      <td>66235</td>\n",
       "      <td>81050</td>\n",
       "      <td>64431</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72311912</th>\n",
       "      <td>1834155</td>\n",
       "      <td>32767</td>\n",
       "      <td>597</td>\n",
       "      <td>1.002166</td>\n",
       "      <td>1.003610</td>\n",
       "      <td>1.000722</td>\n",
       "      <td>1.005054</td>\n",
       "      <td>54200</td>\n",
       "      <td>66235</td>\n",
       "      <td>82550</td>\n",
       "      <td>64431</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72311913 rows × 12 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "df_order_book_50_idxed.dtypes"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "index                  int64\n",
       "time_id                int16\n",
       "seconds_in_bucket      int16\n",
       "bid_price1           float32\n",
       "ask_price1           float32\n",
       "bid_price2           float32\n",
       "ask_price2           float32\n",
       "bid_size1              int32\n",
       "ask_size1              int32\n",
       "bid_size2              int32\n",
       "ask_size2              int32\n",
       "stock_id              object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "df_order_book_50_idxed.reindex( pd.MultiIndex.from_product([df_order_book_50_idxed['time_id'],df_order_book_50_idxed['stock_id'],new_index], names=['time_id', 'stock_id','seconds_in_bucket']))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.44 EiB for an array with shape (3137407657031741400,) and data type int16",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1d743452900c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_order_book_50_idxed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiIndex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_product\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_order_book_50_idxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_order_book_50_idxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stock_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stock_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'seconds_in_bucket'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py\u001b[0m in \u001b[0;36mfrom_product\u001b[1;34m(cls, iterables, sortorder, names)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \u001b[1;31m# codes are all ndarrays, so cartesian_product is lossless\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m         \u001b[0mcodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcartesian_product\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msortorder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msortorder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\util.py\u001b[0m in \u001b[0;36mcartesian_product\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcumprodX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtile_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcumprodX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtile_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(a, repeats, axis)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \"\"\"\n\u001b[1;32m--> 480\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'repeat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.44 EiB for an array with shape (3137407657031741400,) and data type int16"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#for a given time_id and stiock_id itself, the number of \"seconds_in_bucket\" is not the same. THis is the same across\r\n",
    "df_order_book_50.groupby(['time_id','stock_id']).seconds_in_bucket.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looping through each individual stocks, we can get the past realized volatility as prediction for each individual stocks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def past_realized_volatility_per_stock(list_file,prediction_column_name):\r\n",
    "    df_past_realized = pd.DataFrame()\r\n",
    "    for file in list_file:\r\n",
    "        df_past_realized = pd.concat([df_past_realized,\r\n",
    "                                     realized_volatility_per_time_id(file,prediction_column_name)])\r\n",
    "    return df_past_realized\r\n",
    "df_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\r\n",
    "                                                           prediction_column_name='pred')"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_past_realized_train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's join the output dataframe with train.csv to see the performance of the naive prediction on training set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\r\n",
    "train = train[['row_id','target']]\r\n",
    "df_joined = train.merge(df_past_realized_train[['row_id','pred']], on = ['row_id'], how = 'left')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will evaluate the naive prediction result by two metrics: RMSPE and R squared."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import r2_score\r\n",
    "def rmspe(y_true, y_pred):\r\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))) * 100\r\n",
    "R2 = round(r2_score(y_true = df_joined['target'], y_pred = df_joined['pred']),3)\r\n",
    "RMSPE = round(rmspe(y_true = df_joined['target'], y_pred = df_joined['pred']),3)\r\n",
    "print(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}%')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
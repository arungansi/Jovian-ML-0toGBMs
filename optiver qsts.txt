A question about completeness of time_id in the test set. I notice that some book data doesn't have complete 3830 time ids, such as stock 38. This will bring some trouble if I want to build a model which considers stock interaction, e.g., if stock 38 strongly interacts with 37, I will include the features from 38 to predict 37:

vol_37 @ time_id = F(features_37, features_38, time_id)

For training, we can just exclude some rows with missing time_ids. But for the invisible test set, inference could be a little challenging, because we don't know whether the "interactor's" time_id is missing or not, unless we check the missing time_id and build model on the fly. For example, if we build an offline model vol_37 @ time_id = F(features_37, features_38, time_id) but the corresponding time_id is missing for the stock 38 in the test set, the prediction will throw an error.

Just want to get a general idea how complete the test set is. Can we assume all time_id are available for any stock in the hidden test set?
We are sorry but some of the (stock_id, time_id) pairs might be missing in the hidden test set too. Unfortunately, this is part of the challenge while working with highly granular financial data.
Best regards,
Matteo


It's time series but in competition, I understood, you can not use a lagged features. 99% predictive models don't use order book (because it's too difficult to deal with, it's huge) and use 100500 lagged features which are built only using previous "targets". In this competition, you cannot use them? Only order book?


 the order book is updated every time that a trade happens, but it can change because of other reasons too (e.g. someone inserts an order that is not executed). So at the end of the second the order book might be different from how it was after the latest trade.



why is important normalized prices of the second most competitive buy level?
There is all kinds of reasons. The entries of the order table can be thought of as supply and demand curves for that individual stock. As an idea, you might want to look at the divided difference to get an idea of the slope of the demand or supply curves. The more liquid the stock is near the WAP, the better an estimate this divided difference will be of those slopes


Hi, Could you tell us what is the approximate size of the dataset used to calculate public leaderboard score and what will be the approximate size of the dataset used to calculate the final leaderboard score?
Hi, in both cases loading the hidden feature set should take slightly more than 3 GB of memory. There will be around 150k (stock_id, time_id) pairs.



The prices have been normalized so that they will start from 1 for every (stock_id, time_id). The size in cash terms is prize*size, so the sizes are not comparable after normalization.


1. data for stock_id=1 and stock_id=2 for time_id=5 are data that were taken at same time. Is that correct?
2. "time_id are not sequential". Does that mean, data for time_id=11 could be data that was taken before data for time_id=5? Or Does that 'not sequential' refer to the fact that there is a gap between time_id=5 and time_id=11?
Ans:
1.correct
2. the former: time_ids are not in chronological order


1) Are the time_id picked at random?
2) If not, could you disclose how they are picked and will the live testing data be picked at the same manner?
Ans:the time_ids are picked at random and indeed there may be gaps between them.


I wouldn't use ANY Time Series model because, as stated above, time_ids are **not **in chronological order. Thus, we cannot, in my humble opinion, guarantee any model chronological inputs. Why they did not guarantee this is by far weird, given the nature of the phenomena: forecast volatility


We will make sure the real private test set always has second_in_bucket starting from zero and would like to ask you to re-base the bucket in your code for the public test set. 


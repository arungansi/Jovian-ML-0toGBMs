{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### my-zero-to-gbm-proj-assign"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "inputs = pd.read_csv('inputs.csv',index_col=0)\r\n",
    "targets = pd.read_csv('targets.csv',index_col=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sklearnex import patch_sklearn\r\n",
    "patch_sklearn()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.metrics import r2_score\r\n",
    "# def rmspe(y_true, y_pred):\r\n",
    "#     return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))) * 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn.model_selection import ShuffleSplit, RepeatedKFold"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# ss = ShuffleSplit(n_splits = 5, test_size = 0.25, random_state=11111)\r\n",
    "ss = RepeatedKFold(n_splits=3, n_repeats=3, random_state=11111)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\r\n",
    "poly = PolynomialFeatures(3)\r\n",
    "poly_inputs = pd.DataFrame(poly.fit_transform(inputs)).astype('float32')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from lightgbm import LGBMRegressor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# explicitly require this experimental feature\r\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\r\n",
    "# now you can import normally from model_selection\r\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\r\n",
    "from sklearn.model_selection import HalvingGridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def train_and_evaluate(X_train, train_targets, X_val, val_targets, **params):\r\n",
    "    model = LGBMRegressor(random_state=11111,n_jobs=-1,silent=False,metric='rmse',force_col_wise=True,**params)\r\n",
    "    model.fit(X_train, train_targets)#,early_stopping_rounds=5,eval_set=(X_val,val_targets))\r\n",
    "    pred_train = model.predict(X_train)\r\n",
    "    pred_val = model.predict(X_val)\r\n",
    "    train_R2 = round(r2_score(train_targets,pred_train ),3)\r\n",
    "    train_RMSPE = round(mean_squared_error(train_targets, pred_train,squared=False),3)\r\n",
    "    val_R2 = round(r2_score(val_targets, pred_val),3)\r\n",
    "    val_RMSPE = round(mean_squared_error(val_targets, pred_val,squared=False),3)\r\n",
    "\r\n",
    "    # train_rmse = rmse(model.predict(X_train), train_targets)\r\n",
    "    # val_rmse = rmse(model.predict(X_val), val_targets)\r\n",
    "    return model, train_R2, train_RMSPE, val_R2, val_RMSPE\r\n",
    "#n_jobs = -1 means that use all the available threads in that machine where the alogorithm is running "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "modelc = LGBMRegressor(random_state=11111,n_jobs=-1,silent=False,metric='rmse',force_col_wise=True)#,bagging_freq= 5, bagging_fraction= 0.75)\r\n",
    "param_grid = {\"boosting_type\": ['gbdt','goss','dart'],\r\n",
    "              \"learning_rate\" : [0.1,0.01,0.001],\r\n",
    "              \"n_estimators\": [100,300,500],\r\n",
    "              \"min_child_samples\": [10,20,30],\r\n",
    "              \"min_split_gain\": [0.0,0.001,0.1],\r\n",
    "            }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "models = []\r\n",
    "model_params = []\r\n",
    "grid_params = []\r\n",
    "for train_idxs, val_idxs in ss.split(poly_inputs):\r\n",
    "   X_train, train_targets = poly_inputs.iloc[train_idxs], targets.iloc[train_idxs]\r\n",
    "   X_val, val_targets = poly_inputs.iloc[val_idxs], targets.iloc[val_idxs]\r\n",
    "   X_train = np.ascontiguousarray(X_train).reshape(-1,10)\r\n",
    "   train_targets = np.ascontiguousarray(train_targets).ravel()\r\n",
    "   X_val = np.ascontiguousarray(X_val).reshape(-1,10)\r\n",
    "   val_targets = np.ascontiguousarray(val_targets).ravel()\r\n",
    "   grid_search = HalvingGridSearchCV(modelc, param_grid,n_jobs=-1,verbose=1,return_train_score=True,random_state=11111,factor=3,min_resources=1000,error_score='raise').fit(X_train, train_targets)\r\n",
    "   model, train_R2, train_RMSPE, val_R2, val_RMSPE = train_and_evaluate(X_train, \r\n",
    "                                                   train_targets, \r\n",
    "                                                   X_val, \r\n",
    "                                                   val_targets, \r\n",
    "                                                   **grid_search.best_params_)\r\n",
    "   models.append(model)\r\n",
    "   model_params.append(model.get_params())\r\n",
    "   grid_params.append(grid_search.best_params_)\r\n",
    "   print('Train R2: {}, Train RMSPE: {}, Validation R2: {}, Validation RMSPE: {}'.format(train_R2, train_RMSPE, val_R2, val_RMSPE))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 1000\n",
      "max_resources_: 285954\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 243\n",
      "n_resources: 1000\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 81\n",
      "n_resources: 3000\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 27\n",
      "n_resources: 9000\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 9\n",
      "n_resources: 27000\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 3\n",
      "n_resources: 81000\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 285954, number of used features: 9\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.003881\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 285954, number of used features: 9\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.003881\n",
      "Train R2: 0.486, Train RMSPE: 0.002, Validation R2: 0.485, Validation RMSPE: 0.002\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 1000\n",
      "max_resources_: 285955\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 243\n",
      "n_resources: 1000\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 81\n",
      "n_resources: 3000\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 27\n",
      "n_resources: 9000\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 9\n",
      "n_resources: 27000\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 3\n",
      "n_resources: 81000\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 285955, number of used features: 9\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.003881\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 285955, number of used features: 9\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.003881\n",
      "Train R2: 0.488, Train RMSPE: 0.002, Validation R2: 0.482, Validation RMSPE: 0.002\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 1000\n",
      "max_resources_: 285955\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 243\n",
      "n_resources: 1000\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 81\n",
      "n_resources: 3000\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 27\n",
      "n_resources: 9000\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 9\n",
      "n_resources: 27000\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 3\n",
      "n_resources: 81000\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 285955, number of used features: 9\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.003879\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 285955, number of used features: 9\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.003879\n",
      "Train R2: 0.489, Train RMSPE: 0.002, Validation R2: 0.481, Validation RMSPE: 0.002\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 1000\n",
      "max_resources_: 285954\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 243\n",
      "n_resources: 1000\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 81\n",
      "n_resources: 3000\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 27\n",
      "n_resources: 9000\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 9\n",
      "n_resources: 27000\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 3\n",
      "n_resources: 81000\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 285954, number of used features: 9\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.003877\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 285954, number of used features: 9\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.003877\n",
      "Train R2: 0.486, Train RMSPE: 0.002, Validation R2: 0.486, Validation RMSPE: 0.002\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 1000\n",
      "max_resources_: 285955\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 243\n",
      "n_resources: 1000\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 81\n",
      "n_resources: 3000\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 27\n",
      "n_resources: 9000\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 9\n",
      "n_resources: 27000\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 3\n",
      "n_resources: 81000\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 285955, number of used features: 9\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.003887\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 285955, number of used features: 9\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.003887\n",
      "Train R2: 0.489, Train RMSPE: 0.002, Validation R2: 0.48, Validation RMSPE: 0.002\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 1000\n",
      "max_resources_: 285955\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 243\n",
      "n_resources: 1000\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 81\n",
      "n_resources: 3000\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 27\n",
      "n_resources: 9000\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 9\n",
      "n_resources: 27000\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 3\n",
      "n_resources: 81000\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 285955, number of used features: 9\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.003877\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 285955, number of used features: 9\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.003877\n",
      "Train R2: 0.488, Train RMSPE: 0.002, Validation R2: 0.482, Validation RMSPE: 0.002\n",
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 1000\n",
      "max_resources_: 285954\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 243\n",
      "n_resources: 1000\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 81\n",
      "n_resources: 3000\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "modelc.feature_importances_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_gd_par = pd.DataFrame(grid_params)\r\n",
    "df_md_par = pd.DataFrame(model_params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_gd_par"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_md_par"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c63917b8b28bafe5244404bd45a1b251dbf5878a4a81ced2ec89f689e03027c"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}